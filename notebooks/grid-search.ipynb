{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold \n",
    "from sklearn.preprocessing import binarize\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score\n",
    "import instructions\n",
    "import handling_outliers as ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.split(os.getcwd())\n",
    "data_directory = os.path.join(path[0], 'data\\\\raw')\n",
    "#data_directory = os.path.join(os.getcwd(), 'data\\\\raw')\n",
    "\n",
    "\n",
    "X = instructions.load_csv(data_directory, 'train_data')\n",
    "y = instructions.load_csv(data_directory, 'train_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binar = binarize(y)\n",
    "y = pd.DataFrame(np.ravel(y_binar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_scal = scaler.fit_transform(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 2)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpca_2d = KernelPCA(n_components=2, kernel=\"linear\")\n",
    "\n",
    "X_kpca_2d = kpca_2d.fit_transform(X)\n",
    "\n",
    "X_kpca_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Figure object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Marta\\Desktop\\Studia\\CDV\\IV semestr 2022L\\Wykorzystanie Pythona w uczeniu maszynowym\\ml_project\\project\\ML_PROJECT_2022\\notebooks\\grid-search.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Marta/Desktop/Studia/CDV/IV%20semestr%202022L/Wykorzystanie%20Pythona%20w%20uczeniu%20maszynowym/ml_project/project/ML_PROJECT_2022/notebooks/grid-search.ipynb#ch0000032?line=0'>1</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Marta/Desktop/Studia/CDV/IV%20semestr%202022L/Wykorzystanie%20Pythona%20w%20uczeniu%20maszynowym/ml_project/project/ML_PROJECT_2022/notebooks/grid-search.ipynb#ch0000032?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mscatter(X_kpca_2d[:,\u001b[39m0\u001b[39m],X_kpca_2d[:,\u001b[39m1\u001b[39m], c\u001b[39m=\u001b[39my[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable Figure object"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.figure()\n",
    "\n",
    "plt.scatter(X_kpca_2d[:,0],X_kpca_2d[:,1], c=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca_3d = KernelPCA(n_components=3, kernel=\"linear\")\n",
    "X_kpca_3d = kpca_3d.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marta\\AppData\\Local\\Temp\\ipykernel_9588\\3861567227.py:4: MatplotlibDeprecationWarning:\n",
      "\n",
      "Axes3D(fig) adding itself to the figure is deprecated since 3.4. Pass the keyword argument auto_add_to_figure=False and use fig.add_axes(ax) to suppress this warning. The default value of auto_add_to_figure will change to False in mpl3.5 and True values will no longer work in 3.6.  This is consistent with other Axes classes.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter3D(X_kpca_3d[:,1],X_kpca_3d[:,2], X_kpca_3d[:,0], c=y[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x264824b26b0>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(X_kpca[:,1],X_kpca[:,2], X_kpca[:,0], c=y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>index=%{y}<br>color=%{marker.color}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           0,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           0,
           1,
           1,
           1,
           1
          ],
          "coloraxis": "coloraxis",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          96789.87057812855,
          4749.553276848235,
          10679.88556537604,
          -27735.864238144157,
          -3049.3100398281313,
          45019.99692392101,
          -26187.475352081812,
          18287.973047021704,
          -51604.99167903124,
          315.35853049607624,
          37662.33613938765,
          -41149.915346306116,
          30136.99597044095,
          9129.430287661427,
          -31541.7092780051,
          41325.05473669899,
          24092.06506196751,
          56081.081416312256,
          28766.646399073354,
          49175.715153933255,
          85295.7582312012,
          136063.75556501324,
          13504.112234927361,
          74336.26107062612,
          -106911.510272041,
          9216.60231378438,
          -12006.400111196685,
          -170051.9333090272,
          -28910.104053034596,
          326.90734874536946,
          -5401.483755521245,
          38475.113236456215,
          75369.6263787772,
          -26951.874968171167,
          54432.17013402699,
          52953.11933992465,
          -25220.367611647223,
          -64951.25426518863,
          6736.165320713423,
          -57307.50825818092,
          105212.03933640309,
          4573.28796969132,
          -52787.20028087837,
          -71941.85148579115,
          -18531.534100945308,
          -26494.550729810155,
          92686.93695690749,
          -79341.60644430305,
          -41542.47667748066,
          2915.8332648363926,
          44203.13857381282,
          5816.482984845016,
          83500.97125670468,
          44151.11855789088,
          31926.000805203465,
          -17564.283893028572,
          40066.86565088127,
          -22171.104497821052,
          -45755.90137431745,
          154018.5482339575,
          65486.24638488395,
          46979.85866964784,
          111501.05108487533,
          -48529.99616855735,
          -45723.63179879246,
          -102424.83530111924,
          -68731.52037934665,
          -27585.370049739235,
          -33209.812104327706,
          -58538.05498006866,
          -1240.936705730009,
          5486.9065860714945,
          85970.08679463304,
          -26022.24955787675,
          35743.03093150819,
          28222.756941477528,
          -5114.294737354297,
          141981.57371047267,
          -90151.26030619064,
          35746.39261788898,
          -39852.698410246965,
          6454.529135338851,
          115883.61506165045,
          -4093.9953707739537,
          11895.890290868592,
          -12581.20243386722,
          -77882.97003754141,
          -15322.628649822307,
          76248.27207697598,
          39345.21988595868,
          87147.41068029369,
          -61649.123517673266,
          -9928.291324788637,
          54004.20712182284,
          -9898.353707878858,
          -20240.15063110322,
          73137.9010869488,
          -48979.409015205536,
          3251.1773485109297,
          -69476.15396293822,
          86923.58652203299,
          -65930.37906540933,
          -74632.86254019491,
          28148.344914823723,
          -8872.589006410532,
          10536.509045936134,
          -12296.843083910666,
          -84534.21363444484,
          89343.62657643578,
          94648.81379330497,
          -166651.95101816583,
          -5051.878039511086,
          40465.503728575946,
          -23313.857455511654,
          138102.2570907543,
          -1707.309351137566,
          34388.238542282284,
          16429.061620637345,
          2966.470822208104,
          32930.01407315112,
          -56411.747634783016,
          6850.992919968631,
          3030.9240406236104,
          -37855.50775590364,
          95386.45912203006,
          34208.83370581561,
          -13803.44740690216,
          83216.58986874264,
          39152.17817634408,
          -52315.404473883995,
          -21154.15924962998,
          40344.965177324266,
          -18556.942508386684,
          -22037.97195459406,
          -99564.62840945675,
          -48692.17495334449,
          59036.74969952265,
          50356.61369924252,
          -23320.320181487416,
          -20066.194850315253,
          1178.9719601652123,
          14373.658300488753,
          61173.13936003794,
          -11403.713998764477,
          -3713.9973023543885,
          -70370.97824662608,
          -63688.47932512731,
          -52170.56658232379,
          -44629.093345374014,
          -92836.80138757468,
          68814.06466845512,
          -54055.6464167471,
          -104095.72705803331,
          16650.35801623764,
          55722.92922562271,
          -25788.46921421456,
          -66304.7150442319,
          34735.867228011244,
          56278.9079619962,
          80695.79804675034,
          -41069.011716771354,
          -22785.461036090768,
          63848.05068285089,
          -5512.90924448007,
          -9115.400310483572,
          52607.7678874645,
          -19963.72555459756,
          -31898.405465388285,
          -70191.96737628621,
          49281.28231354504,
          -44757.05628333193,
          -1716.698598772819,
          -20843.34733735522,
          204.1596517213733,
          -26644.423727292207,
          64526.754342585045,
          66130.38459817345,
          -14560.842228459036,
          22248.799156200424,
          -12217.016496727858,
          85307.45007148822,
          -54832.061629038246,
          24451.802524456292,
          -56632.75977672333,
          -34024.59823656581,
          -41764.685199723615,
          48417.73748066141,
          82599.19246725872,
          17189.184963570136,
          85694.00150867415,
          -30657.182293417703,
          23811.551922112965,
          -89560.3355332132,
          108100.57376560621,
          3514.575132226526,
          -21154.49772813318,
          18023.801172878855,
          27561.521804732965,
          -19608.54838095153,
          37643.52600213487,
          -131315.58500005264,
          14982.732336809837,
          -136690.59105649224,
          -27472.78891959959,
          62589.25772926575,
          32438.827895089347,
          -43789.4481895851,
          10894.487074444538,
          31051.494864959655,
          120307.39832566709,
          45612.21347525767,
          -54132.96987224215,
          -94018.35581445637,
          23378.54377011937,
          -5061.882547819447,
          36016.12991563972,
          -11078.867021643668,
          -42303.64142767193,
          46337.43192730495,
          34552.95073805604,
          20160.441352357448,
          -74389.09637246725,
          -53595.55396570343,
          93967.40223012512,
          -90233.93320520091,
          12520.927657757562,
          -17401.05738590877,
          27367.7697240048,
          44700.55978534827,
          108021.88432063478,
          14331.703510292145,
          -13125.576933858512,
          -74395.11129829366,
          -16671.54248565198,
          41794.52151132928,
          -7024.3221196636905,
          -34887.55357147347,
          -24504.454273416846,
          356.64639180282586,
          35179.99162188809,
          -93371.88088224737,
          7073.209522928409,
          92.07973860828075,
          -24814.35334096532,
          -69058.87224138845,
          -1769.6608638812636,
          13055.118577844829,
          -81326.20104030373,
          -8020.330923260397,
          -38985.040186590624,
          -80876.47400796977,
          21555.081341851273,
          54401.025007066135,
          70196.26732936263,
          -74406.2588923187,
          -4300.049808584364,
          -78818.3363748109,
          -38757.0069728656,
          -21663.516141301,
          -106198.72983993024,
          -77729.01681729556,
          22833.772655788107,
          -27262.069859988947,
          118097.75866565321,
          45795.69761112802,
          -60948.063795002905,
          -58385.73414434224,
          23008.521204020133,
          -69484.35984759488,
          112219.49133549428,
          20991.8402219377,
          -59763.2175855596,
          40854.780733571126,
          -23710.94481059615,
          -4527.193426741344,
          24893.10870068713,
          67901.38478897167,
          -69025.18452537927,
          -14312.520786812558,
          -44888.74042437913,
          -15600.838511940798,
          -19895.1879610777,
          -260.30492999554815,
          -10658.780888445053,
          7128.609491345531,
          -36802.27849826667,
          36403.3426259799,
          -124066.32620801848,
          94796.33003996656,
          27549.8284520324,
          119743.93933978265,
          14726.03857402642,
          32833.774240970124,
          42768.75623396814,
          -10946.162224468186,
          6044.813470743508,
          15567.755612043073,
          108036.99829624042,
          -5786.8310407487,
          -15631.288955858707,
          -62487.113740687615,
          -28918.609079644237,
          -35903.98493744023,
          64687.31983960498,
          41570.28636612448,
          108350.58652186407,
          -33925.04791225019,
          -37377.68800825736,
          5569.499992592603,
          44560.03718394613,
          37963.4881729842,
          18180.574461236967,
          -56441.22198529076,
          6920.977589083408,
          -3519.1287733845875,
          38590.902436720484,
          64183.3575959147,
          -85000.86698181472,
          -44035.46613309588,
          -5437.3413663215,
          71060.03812096202,
          8998.951409600648,
          70434.95333018701,
          -22361.650020311088,
          -109749.03630951628,
          40604.00166447287,
          87324.2773276847,
          -44054.83071747929,
          34942.66995148665,
          -59081.11557214913,
          32473.312746039446,
          67104.09393525899,
          66101.20727449656,
          -59759.56937113638,
          110905.30809635676,
          -73907.77663407095,
          -88141.98835875731,
          -27313.01435574003,
          -7769.198666045735,
          90904.38005578781,
          -45658.13588654465,
          -42569.733368456116,
          83380.19005157282,
          27969.25716563686,
          -21347.804674943833,
          138757.15747046904,
          -77311.2004270967,
          48308.72935476161,
          42542.39840110365,
          40995.241658317755,
          39005.0902893489,
          33049.4562435502,
          -92673.30269219242,
          -22681.74588926043,
          -70159.56883657823,
          -44181.942928738914,
          63045.25257212765,
          -34375.47959131782,
          -7702.300380415566,
          45512.15905733045,
          -89210.14220521746,
          13554.484363597086,
          31886.822908336824,
          77494.31480817996,
          -26428.439917410004,
          -2447.0858794927253,
          -57348.28867562088,
          12171.19353189279,
          12958.484200550545,
          -43352.94635831615,
          -65062.69389574704,
          -14216.406812132136,
          -70231.6019803293,
          -18315.181801867362,
          23333.538266665433,
          -66737.24815221694,
          18438.781057042623,
          -9015.579078414848,
          -15363.965720497421,
          1558.6321783280675,
          20180.75750796995,
          -42819.72351289236,
          57074.26073353229,
          -62423.03522817184,
          -6403.646577739501,
          2986.6619290844747,
          -95170.43053179387,
          67927.88458909452,
          -127332.81850312078,
          70102.31805406364,
          6054.908158085317,
          -48586.65892647479,
          -40195.466373876625,
          -57543.61444909107,
          -15399.106376513771,
          47159.12155226714,
          19243.4384733911,
          -2551.222130927698,
          67336.13102728124,
          74444.27595645102,
          13813.390180677168,
          22003.710258441453,
          -20942.475256028636,
          61883.820688751475,
          17582.35655191648,
          7245.714641983019,
          3885.0360876594077,
          -41005.91945576518,
          42274.11332055471,
          -22797.391076392156,
          -52628.95877345975,
          10277.267092700042,
          -112252.69551303373,
          54626.69463812154,
          50348.691364436425,
          -27526.79682184279,
          26332.07057950655,
          73722.95073311286,
          -30665.31856951398,
          -16645.521862806778,
          45082.87537618611,
          43447.249642601695,
          -33957.79109155084,
          9535.41184634304,
          -26282.007914679958,
          -68562.7820347892,
          25681.980735552246,
          22876.544280734786,
          -1408.6038319819909,
          16125.589541302692,
          -35594.089474886394,
          -40185.112926334186,
          90539.25143503414,
          -127527.33096283197,
          -137615.11038148162,
          -10336.87591287174,
          88610.14018603218,
          -19726.991832373984,
          10156.42661404886,
          37722.218350307085,
          47907.63394909124,
          9730.737966139366,
          19281.192264655216,
          70482.91317489621,
          -18325.185702531697,
          21751.25027042941,
          51173.84517770253,
          32393.560149323504,
          -44680.92060414976,
          64980.07360367443,
          67413.75869813115,
          -35705.37529459796,
          12556.036924273534,
          -34171.00688274131,
          -113500.14365799497,
          -42043.2779114639,
          -214385.3718681687,
          -4562.195548163273,
          -97650.86187573656,
          87556.71547501464,
          28760.70333022126,
          41657.43480842651,
          38621.69664895556,
          35193.0715008422,
          42627.1828448517,
          40189.42249329587,
          32899.80310946358,
          -13581.345103575271,
          -15130.16859347356,
          18813.928062484934,
          136933.7105826126,
          14582.504181853184,
          58451.457273987326,
          -53091.8154036995,
          25473.582523933117,
          47356.34880033547,
          -73748.99571712484,
          -31055.88380665543,
          -50484.68831961531,
          -73289.07103032543,
          5531.679145053999,
          -6339.042350949207,
          63462.667751366134,
          35367.433794485674,
          -53163.24124912712,
          -50713.31499584335,
          -71618.369242911,
          -130019.11450068772,
          21714.131922155277,
          -30512.2661544607,
          22634.334780357225,
          -96596.13126517058,
          24869.10114347586,
          50920.11426842337,
          119603.22444154535,
          31729.690747618224,
          64474.97959215638,
          49681.665753577414,
          13.256035516519214,
          -21851.116326534186,
          2306.0259571533265,
          87051.6566241837,
          -38136.74731462436,
          -27612.0614780287,
          -16682.32753569953,
          66642.98053953437,
          4282.640390071763,
          -411.7713787940226,
          7227.639656039386,
          -33502.32771997631,
          89334.29601988617,
          5153.16618460356,
          43330.03032362517,
          -144135.5408208962,
          6102.586927246045,
          23298.599249186795,
          9267.270605633918,
          48035.72621880557,
          -31618.27049786591,
          -15347.55176986789,
          -43910.370253500325,
          -40547.04636304415,
          -28829.459070880865,
          10138.64984421439,
          22513.59518245728,
          46389.71075318342,
          61490.506117349825,
          -10841.51197277416,
          127144.30770972953,
          -49750.200380985116,
          -99291.81061556238,
          -2460.939166956132,
          39822.05417781482,
          -70308.10709680339,
          -40093.568977285046,
          -25232.818165457295,
          6376.563213846134,
          -52.00314801705485,
          -81914.24415682643,
          -79640.52571702818,
          -47083.796187987246,
          7925.435632254018,
          56475.89901529088,
          48961.85682189927,
          -2453.1599880798217,
          152335.28561018626,
          -40274.946709560485,
          -4826.264399382511,
          -134870.2109690835,
          -26902.722528403996,
          -36446.69592139779,
          53786.22979794268,
          -41148.068449320475,
          -29398.064942436846,
          1337.3144672326405,
          61452.42389464432,
          26624.832150974777,
          -18140.56928449745,
          24639.91268795307,
          -69298.59733969612,
          -4426.574148354797,
          -51633.046239379975,
          3123.836959505774,
          22416.67518926543,
          -5942.31918486832,
          -4233.849860340121,
          25256.903139310565,
          -51724.93774627543,
          39457.012786514664,
          -83942.35199778671,
          74480.28220400012,
          -13962.148068697828,
          95359.80319914718,
          72753.23226169996,
          -1906.046703620424,
          -8411.597727032462,
          -48403.30616051788,
          -37926.8068839201,
          -46159.7624078726,
          95751.0906196899,
          -20844.665430037578,
          23320.942077502852,
          32554.61982515597,
          -129011.98254965947,
          -2918.985712656541,
          22735.952601479792,
          28466.487381028244,
          -44834.40319730726,
          156405.65004673606,
          3052.400685217276,
          -15935.187629380565,
          -8595.536256203108,
          -64457.08135846499,
          -36800.394947968125,
          12050.098814154782,
          -27794.45642843244,
          22855.31095650083,
          -33477.15065274404,
          7513.61860799572,
          -91349.28473974383,
          37704.56959434485,
          -36241.201314033075,
          -64259.64877050309,
          54010.39483413947,
          60194.01410781129,
          85485.45361210754,
          8358.715378884104,
          -53821.74423239192,
          569.8800688839216,
          -15940.006206858758,
          25120.17388300168,
          51987.75091168852,
          -89165.26944312967,
          -69425.35685490008,
          29697.04920584803,
          -16368.61911600024,
          32507.303822702343,
          14887.133387199254,
          41196.12763635363,
          -73698.07133333074,
          -150223.05988009583,
          6853.931802988722,
          -73923.56924028479,
          -3045.254778842037,
          -15976.370888864867,
          73187.6883043796,
          -33609.660876623326,
          -50162.03393558781,
          56598.30826494976,
          -15147.658127449575,
          -11429.733944825328,
          -64425.83183871311,
          -27458.933936069083,
          -18384.057522368308,
          418.6149868098716,
          -22315.05526293783,
          48230.06876740987,
          -8630.527335316574,
          99908.36743661013,
          -51689.15993294527,
          106573.34578485644,
          -2260.164671836794,
          26576.549190956976,
          99702.45358335091,
          72538.40397081949,
          65291.34639444795,
          12468.939027420269,
          -78964.3117338353,
          -78116.122889456,
          64140.748910010516,
          -4113.259077548769,
          -6596.745584446222,
          51395.72387517664,
          -28084.009230202584,
          23752.160253270984,
          35537.09453254186,
          71851.50021096754,
          -21842.119779345347,
          -49509.507277725636,
          52572.909642049955,
          -87939.6487368519,
          -22256.907564674937,
          32774.98127380286,
          5333.253474202516,
          9935.249283109182,
          -49340.96176966619,
          5754.713384050595,
          -114805.72086489896,
          -35077.686475534974,
          76440.2037596378,
          82477.27180989194,
          -38277.45598495586,
          -40214.26363271112,
          -82094.51836596917,
          69714.29839813062,
          -61718.00341278408,
          87007.52718284438,
          35633.32507987356,
          -86614.01351230362,
          62110.93854609134,
          7856.161017617196,
          40517.99133342115,
          -9645.16261103167,
          13721.86583177665,
          -64181.697487613295,
          113045.96970856407,
          -17616.5424757981,
          5395.519198943619,
          74655.96224968825,
          -11398.168839833446,
          -1336.6590360945531,
          -28438.963675346935,
          25404.901577663495,
          -3821.7463652856472,
          -44255.48440261775,
          50602.983468286795,
          -19351.972824976114,
          21841.0009512297,
          82805.94683478751,
          -3238.371030568636,
          13980.477195456357,
          6021.29664051788,
          85674.98590695254,
          13070.02805013341,
          116011.41456600894,
          -27906.913903136734,
          88289.29242574653,
          -65944.82129441731,
          24105.981005480295,
          -47372.51142812915,
          -11523.009564250326,
          -24185.54176099026,
          -15330.656601231558,
          41054.111649446546,
          -87589.66079670444,
          -31487.856073102954,
          -32202.724937125462,
          -111023.36945068862,
          111042.55201762677,
          14019.857789341733,
          -19622.590621991734,
          24699.183469763408,
          -61017.18200447843,
          -20600.30203247624,
          102191.09301083953,
          -19691.202336139653,
          1021.6383729978597,
          -6126.592431887247,
          -144864.80917045646,
          40786.76020018707,
          -72639.46237357159,
          -23038.111729619337,
          15854.199433855612,
          21868.434237481455,
          18607.333445185985,
          44548.524345595266,
          26397.957090898853,
          7320.718503241053,
          34236.66199186499,
          22913.589942026338,
          92675.75124638704,
          27799.040671081904,
          55812.717918542396,
          6796.467560346457,
          -87695.75965317331,
          164801.86514818406,
          -31832.853891827883,
          -20372.93062728962,
          -77031.94260675118,
          33980.5289864164,
          48549.72624516087,
          91681.97691062797,
          -78818.15220758884,
          -32321.461889455182,
          101070.35616132001,
          54561.8037008917,
          -23918.546311052327,
          -86863.61887457025,
          35603.344745342896,
          -74264.17402770798,
          -72207.4485229929,
          -8195.730012261296,
          84665.80060448304,
          4166.262458613253,
          -8916.108889712874,
          105514.16847491883,
          65554.82988945248,
          -4439.443243378288,
          56849.88505913943,
          567.177759321701,
          -17211.923102736513,
          12852.208506405224,
          49275.83679339486,
          82168.98458558148,
          29379.309538263788,
          -52059.825847873995,
          -146456.37945726322,
          -64555.99774976148,
          43940.26022414894,
          -35021.25140584903,
          56763.13789763975,
          36931.55323823656,
          17575.560915440634,
          -9822.491894921686,
          26245.938238629915,
          -33164.879343631444,
          10221.087116240855,
          34831.29754703181,
          87691.21150939514,
          43495.26306339846,
          -78162.643529184,
          14548.705202295498,
          -72414.98326210916,
          -50932.584000261006,
          -55498.07760316033,
          -41714.17875876687,
          4588.330975315366,
          8913.339016177855,
          -39550.74598719487,
          7861.001037656293,
          -102505.92667093706,
          118643.10227867184,
          -76263.73692321095,
          62139.579017652184,
          -88203.63130372048,
          20683.16781537363,
          -39314.15238036242,
          -10114.74794461975,
          -72705.25335849682,
          68094.56989460037,
          66700.21727850265,
          -4348.5368860083945,
          1134.0754201807977,
          -43639.468574525636,
          100314.00892116207,
          31566.55673951435,
          15243.967257785662,
          36116.29317261079,
          -8751.931630778938,
          47597.869140566974,
          -36472.17114337257,
          38288.70246514867,
          -116305.75147198071,
          -57257.70484267666,
          -79784.95315452115,
          -47883.36310912575,
          -7842.404702878223,
          -66575.81788151909,
          60657.40436438354,
          39392.816862022984,
          -46708.68152180827,
          41810.31985495226,
          18682.356906867928,
          -19351.065602265935,
          40809.79515284072,
          125834.36133520707,
          -156823.46315361152,
          -39002.741598637374,
          66045.70843316344,
          55610.56905366581,
          -6733.552765199342,
          -102424.75328472441,
          -18841.13545017525,
          35721.70736152871,
          29578.11633115116,
          23707.636243149314,
          36123.30160721776,
          6155.031783189727,
          85448.54704351818,
          17816.448281404406,
          82070.86721570438,
          16271.699682003557,
          38767.92673760158,
          -30486.02780472224,
          -16144.79262947741,
          -23243.78440645575,
          30005.693865585374,
          19495.59123864852,
          18189.455169971192,
          97997.25206711801,
          19837.992259289258,
          77345.12214972921,
          -45000.449402703845,
          -8779.964813993303,
          22371.73451336801,
          -51710.764341398615,
          80876.98824644872,
          9500.707681625321,
          -88345.87778291001,
          -88750.62196566447,
          4817.676102963455,
          68615.9643422337,
          40580.69346989442,
          -8966.079504365576,
          83220.46625450783,
          26429.86133199254,
          -50034.244860036146,
          21556.58698933873,
          -8362.167657368964,
          -117924.48567980799,
          -154238.60816815507,
          -1511.6230262807046,
          83835.0171348359,
          -31664.51539140478,
          96133.24114843472,
          28241.23014152998,
          12973.696317543947,
          -44469.657158153896,
          35320.52056848384,
          8952.470868907001,
          1469.9775798676005,
          -79494.77687971565,
          127098.58562820363,
          54878.63889074986,
          -54725.58312051713,
          -30186.66878408028,
          48272.93808068001,
          -6427.37912285387,
          50752.93068284568,
          27783.84009107879,
          15836.025185399525,
          -5056.490320183294,
          -61118.556215875564,
          22695.8810532861,
          -58464.16158256895,
          -8385.210659481836,
          17446.636378489293,
          -43601.18154007592,
          93105.22342212229,
          -5461.543060571318,
          -15305.415093903977,
          -4995.630711171895,
          48539.09455692979,
          18379.596141481503,
          24320.76727715184,
          -5643.611504040559,
          -106934.35726843556,
          -45247.44529304555,
          67020.04121622481,
          -28061.915599822074,
          152264.17533160973,
          -21331.61778671412,
          -126858.12914592244,
          -19050.62544367955,
          57376.26137630877,
          -80334.51038797243,
          13550.914346503787,
          28265.37134257309,
          38088.482403561786,
          6696.070697029889,
          27845.376341435604,
          -34941.99346828316,
          74285.63605934256,
          -85223.90027899164,
          29830.33465882902,
          -77808.86260121907,
          18765.428095406707,
          8426.219512701027,
          68385.85701550466,
          -5738.249937985734,
          24517.095218453764,
          -99668.25958871565,
          -13042.69959636065,
          -149478.9521184807,
          -40586.18361583221,
          -26375.57415218819,
          19610.892822140788,
          2839.9120521699015,
          77313.65158434036,
          46783.70441565518,
          70335.99301129632,
          -53028.76414458468,
          -28604.979020444185,
          -49790.10838093533,
          -25230.548197906413,
          -28272.803932527728,
          106709.62267271752,
          19953.081635822615,
          -37401.482011119486,
          15636.908124000343,
          3022.725699535091,
          75413.09748851007,
          18355.38255289992,
          91435.57646684887,
          -2408.2040264205643,
          54390.856475462606,
          -41721.62026416081,
          18783.177185056,
          -38099.97559202944,
          -15734.393214925118,
          34101.35416454709,
          62192.5627341408,
          -18079.420625487917,
          -141823.48301945257,
          22131.51624437611,
          98620.25134287594,
          -55974.9589820539,
          62469.69272785835,
          67669.81531718222,
          42603.706624309416,
          10022.678011167429,
          -67438.06319468535,
          24956.577017153228,
          46853.83067358354,
          66375.89107715452,
          38120.37341758851,
          -22740.774240201503,
          -87920.34124299661,
          102641.5133270527,
          -56029.68727827427,
          -23321.01600814208,
          -38761.068730197185,
          76897.55666534408,
          -35470.47514859067,
          91151.16307881697,
          16965.32334926758,
          34403.23428331691,
          2869.79992577979,
          -27398.597392549105,
          -183.93374873473945,
          -88294.75957606053,
          -26420.300046788576,
          78534.4862384831,
          9608.124925229895,
          -1067.3014688628514,
          124739.09278452116,
          -42460.9519065998,
          -48567.93600437496,
          -62038.93989719171,
          -79677.55421662655,
          -49373.301325095155,
          -19680.93301991233,
          21833.848794508478,
          -30807.128364904907,
          -15290.667357976898,
          -9591.117925702785,
          55338.693684780585,
          47260.98386819958,
          -53173.898519146234,
          -18440.884034607592,
          -58327.10800626464,
          -15448.669098963146,
          748.3765845504249,
          21061.577236498262,
          -45286.53361740185,
          38106.927957884254,
          -17457.540944365563,
          -66223.60871034136,
          42496.32308098916,
          26110.83270590715,
          -24284.05134088028,
          -83311.10309567468,
          125627.15168180985,
          65125.76947969894,
          -45278.55525410052,
          51309.415956751895,
          -13539.43835378354,
          -16477.594231297146,
          65209.78881871842,
          82602.60706906476,
          15684.268166496275,
          23404.930122297737,
          28592.95126861798,
          43277.48035580113,
          -54820.59066582743,
          -94283.91610913462,
          4752.65723673156,
          -30711.145273753067,
          -35767.948773623546,
          5584.586077885724,
          -26363.14354997737,
          -6159.284772780646,
          4118.738132161429,
          -25008.642514192794,
          32267.60497168439,
          -38323.72741981897,
          28877.174992109307,
          12967.845162003025,
          -16698.90298944852,
          -93277.17345228103,
          20896.744495763633,
          -77074.66680977434,
          -6147.1867574787275,
          -98895.75863835048,
          -114145.6979307737,
          -64741.96311743701,
          8195.366975552266,
          -74389.2666441484,
          69324.93312300766,
          87711.2509926881,
          -106999.84250311821,
          -31760.046362260025,
          17190.415716246134,
          50660.889372796046,
          -41772.49815107709,
          -37362.760908085926,
          -3180.0406799282323,
          -14861.950100049711,
          78880.97860058102,
          -52298.55602033169,
          -117226.13339107628,
          63074.43849587929,
          -98752.96363498044,
          18317.139755667973,
          45484.87411299732,
          25957.338638895293,
          48256.694505042324,
          104415.05494962308,
          37388.79875433099,
          -120996.82196571199,
          62833.74072971044,
          -15721.83873498194,
          35303.52259551681,
          -35577.86613062092,
          78285.43298385495,
          27625.667275911648,
          -48392.74949302893,
          -38810.96563114602,
          -57404.65536924232,
          50977.32991434551,
          52498.62686049892,
          10118.467313810028,
          25548.230207841883,
          -36706.03385414571,
          -42938.824149616994,
          7481.323474517225,
          2966.423512787191,
          69323.63467923837,
          -87971.93073133237,
          -94616.73650351564,
          -82704.25853379027,
          82296.21841883003,
          48056.14276521989,
          -26816.092689397068,
          -14757.474917041984,
          -17921.782699269676,
          -12821.008282170851,
          6867.045223975791,
          51571.59706547834,
          38620.25700323304,
          -3834.045014956429,
          -6197.106851481304,
          -100230.45221466752,
          66058.1127439049,
          -3761.6058778989554,
          3699.9207994629696,
          159692.9456002732,
          35713.79967418231,
          -15069.20886070975,
          -23143.76025849262,
          -51917.426256050356,
          -115636.23245401493,
          -32903.95852767003,
          -39622.23951390069,
          51567.42638301455,
          -132575.46399811804,
          -12213.947940052896,
          -79353.3185214378,
          -29257.407404835452,
          72248.67023402051,
          24381.453319686854,
          27881.97808159556,
          -32328.527990235103,
          125202.84341744032,
          -22180.136126155478,
          -51013.1358379283,
          -47995.343338664854,
          3268.4559901128346,
          5090.0530944137345,
          -36309.04096082852,
          -62174.090836049,
          25906.75013857413,
          -93203.81121935109,
          23769.69129124013,
          -67238.06634152871,
          -132217.77547826225,
          -68955.30271896141,
          -76093.20342177122,
          -64811.01514313759,
          -74620.75338787543,
          -11357.376796664377,
          -129208.00168007128,
          -34965.551119022515,
          -10279.13870675988,
          -20808.357301884214,
          -85563.98469064152,
          69244.64820811692,
          14776.533804760958,
          -13134.67003654531,
          35975.28376572651,
          35714.12643491447,
          36698.385154111944,
          -9509.173321186783,
          61572.91002775541,
          38254.93343575822,
          8229.958782805208,
          16413.282799664215,
          -40457.85267342967,
          -20528.271108677603,
          42244.83540465927,
          45505.70485166467,
          -53538.189865635664,
          -59052.32621046579,
          12436.925766247221,
          14837.502865874703,
          -78951.2581821289,
          -66319.52891984535,
          -26551.53081079061,
          -30329.96755118668,
          -26328.24685600287,
          -84526.00357199024,
          38839.72669573708,
          -18693.618292692656,
          22887.163065934907,
          4354.607678301363,
          125431.85805396635,
          -75966.94343794906,
          -89118.37051904689,
          -1688.4200223106775,
          -55121.22575287742,
          -22982.369485716412,
          53058.585780411566,
          -4695.006882798758,
          15653.585049869347,
          -46948.742079159936,
          19945.22657599314,
          -116617.54822883138,
          -28772.823798128982,
          76583.268985679,
          50335.711897324516,
          -5276.2710701324195,
          -12218.407020262895,
          34981.53681883752,
          87637.93552499019,
          -59668.40172776786,
          -101438.00369442227,
          -9886.431858702179,
          52452.87207037157,
          -43642.04336374323,
          55562.64333471075,
          -38596.42720167785,
          -41376.67447828568,
          12108.851416611207,
          -4760.183498956931,
          67972.3382745489,
          19577.121940223067,
          47131.24971512236,
          8750.738945877423,
          -13741.401402962945,
          49504.85371809679,
          -5525.78901952673,
          -91286.98870316849,
          40525.3976494536,
          14029.982329244123,
          45044.32639841172,
          38011.75575711102,
          149649.39325590798,
          29788.865057111037,
          -50571.096194128695,
          3701.989154051792,
          -59622.619600592996,
          99394.40383499682,
          -3327.102202901232,
          44177.36742517489,
          -40758.362585749324,
          97303.32367135739,
          -6622.5235427039,
          72338.91927095933,
          73491.3293494931,
          -110454.04962375967,
          125091.94943453177,
          -10520.401399065646,
          -128555.11808258855,
          9978.15368252871,
          -45141.82369078534,
          -16440.49602698564,
          76283.16252380777,
          5136.205895972822,
          -65816.07137360564,
          17784.143310614978,
          57042.57713768878,
          -5125.085455233646,
          51131.31377548997,
          35220.43022086389,
          -16784.32452104872,
          100627.20428691132,
          -12000.11729182335,
          50113.182819874564,
          7980.0224651354165,
          55468.52800958301,
          -66194.2405076985,
          -135440.15732681938,
          -43339.87693584769,
          22360.38842030421,
          12259.9759379758,
          -113031.2890497121,
          44179.2262312388,
          -94871.2768828773,
          46032.60327060496,
          58295.11157617495,
          -1228.6894105546276,
          -33527.24167640975,
          -64833.493022892995,
          44402.24140786913,
          -105124.81671698246,
          -9267.929097729138,
          -39869.965758662634,
          -290.29378255779903,
          -34420.15166336801,
          -41870.67228227672,
          15265.38598416129,
          -16320.266307406391,
          -211.96599732693872,
          20414.294566797824,
          66620.05913175887,
          45252.171190524416,
          57288.040502193864,
          113415.12986557122,
          -14845.619547161243,
          6350.157238458766,
          -10108.136116131082,
          -77217.29570508034,
          41114.762977906714,
          -67080.00807423568,
          47338.394693437804,
          -56979.09005740685,
          7090.7742134192595,
          -26593.830728668447,
          -83425.50087937767,
          100228.50815275016,
          -5651.091129982761,
          5231.805649531862,
          -50832.387786222636,
          -14627.897632273516,
          71012.82925074606,
          56404.36804426238,
          15899.232357120787,
          39066.32226692623,
          -42345.728187330315,
          -105958.30778630542,
          7297.185779303997,
          -28219.905843851364,
          10250.377692766699,
          -57306.7543336399,
          118955.24713608954,
          22434.439249504896,
          -57556.112262379815,
          -67459.2336893834,
          -21828.474671306754,
          -35669.15836153981,
          44414.51246350126,
          -17252.39705470253,
          -56900.57039695857,
          34177.387079168875,
          1545.5697276612398,
          98727.520096246,
          40783.23151551828,
          -16497.046508878542,
          -71615.56253121096,
          48713.679567668536,
          -12106.959610865171,
          -47226.96470699951,
          -11267.072845315577,
          143730.92239853853,
          -49405.94842622927,
          33889.82451878736,
          3225.8918829249246,
          8662.989664257582,
          68212.24491833996,
          48620.883458131,
          -68281.72485428229,
          41740.55668375974,
          -45524.508092596094,
          -43524.88728545595,
          -10785.037256908337,
          17811.240113493295,
          -4205.076854058355,
          109411.88252824698,
          21619.995674413738,
          -43431.258916784565,
          -62012.75377414898,
          17143.374801504546,
          -36294.21318822282,
          -31338.084707720765,
          38250.99825995857,
          -24194.605279501073,
          -57759.69952837315,
          41123.61528357942,
          11757.950400520253,
          -91264.70367784881,
          41184.89659621994,
          46367.42665879217,
          29464.374618941394,
          7991.784816115498,
          91407.12180870087,
          -31157.4499836214,
          14559.568452279162,
          61331.67255665152,
          92311.86512004386,
          47796.895313158704,
          -26823.334871245177,
          14162.63727348049,
          3480.7846257949864,
          -71404.39834940921,
          25097.71564465153,
          74300.91397498208,
          -45323.13843198969,
          29436.818333716386,
          11151.574649091355,
          59052.175400156324,
          20992.09961957922,
          74121.41514166066,
          -3257.5013926385095,
          45591.58682003118,
          13395.716183156997,
          25784.500804386047,
          99229.30110009044,
          32121.5795083535,
          -7903.938987148519,
          -7578.570805458695,
          73702.75633858555,
          -33125.42172522741,
          23085.61588170932,
          -55792.707396150116,
          -70833.03663407438,
          41312.99588532833,
          89461.87274647443,
          104829.31994007702,
          -3105.065739152379,
          120519.33728565343,
          63364.1929643807,
          38520.740614354014,
          -69412.84718766694,
          -50084.96641277388,
          -23732.753430840094,
          68738.49402417536,
          -28471.00194830735,
          -43997.268836767544,
          65625.28249565294,
          -31636.882391445808,
          22785.06024388159,
          -6468.566080150722,
          67237.62500755985,
          56380.47573632507,
          13854.458276753207,
          -61915.44435302118,
          -31499.212613993303,
          16093.107519426552,
          -60888.15952525502,
          22601.80666256562,
          -57838.54580483809,
          -22164.66647478967,
          22658.535482297768,
          -16046.34325058771,
          41245.74698028965,
          -2198.124618405985,
          5988.703863630445,
          45229.255344700025,
          25293.936198850704,
          -73165.31677043033,
          32850.31803264308,
          -34948.911493277825,
          -46946.355841345394,
          27594.176796645253,
          -44863.617515071004,
          86746.9353685865,
          20867.076409699963,
          41966.21846964333,
          -12544.8977657967,
          21267.313479747998,
          20550.980937004344,
          -27861.527121420346,
          45455.57678374146,
          63977.28340621623,
          33100.102534350044,
          -28123.029004090884,
          -39743.71839414411,
          14066.041947028916,
          70227.24007757124,
          106956.44036562287,
          -64966.30151242919,
          -40691.0896921524,
          -47564.93245068412,
          -26712.490498503048,
          4542.213863351198,
          10125.61809316694,
          -17357.368065024322,
          -48045.77500213168,
          -71438.26547691443,
          54023.70654379127,
          -80258.1088027745,
          76321.78307218489,
          50975.93609071945,
          -108691.07025449211,
          -4421.310564146336,
          -49422.255245520246,
          -113809.89499759792,
          -28502.063688486654,
          -58240.15706053367,
          -170639.93562740606,
          51411.249073372834,
          -51737.161339352875,
          22277.067116711045,
          -41155.66237726461,
          -53166.31091511062,
          -79179.95688168096,
          119527.97040606666,
          -2005.3162811436066,
          -40183.14974899383,
          -54791.29982165861,
          -36832.80768804833,
          24344.80694299748,
          -7263.283008264294,
          -18386.501528337172,
          -44446.191648271844,
          -30981.261271821713,
          -46340.139571413245,
          -155815.5213716161,
          -1824.8180359244423,
          29101.67522083947,
          -26486.876874276095,
          14406.140593077576,
          -25641.023011485107,
          -26411.71173497109,
          6067.710594611137,
          -69070.49807468378,
          -75661.15710590724,
          73843.13041987317,
          2804.6855176634053,
          -128669.7168119384,
          -86435.01236795948,
          73410.73392859529,
          -23696.797197320062,
          -33296.88115288667,
          52649.83973824981,
          56977.013915736534,
          -4425.319913554157,
          19434.46251288677,
          -111767.64224937696,
          973.0009444495165,
          731.0498289248528,
          74131.14832875467,
          -67433.70947229356,
          65864.77143086573,
          -19767.637605645083,
          125237.5411831943,
          -7324.5022815453285,
          -28333.78513987374,
          -35128.492590774964,
          19946.887398020648,
          -3470.428293127641,
          -36691.11446429538,
          43145.30280078246,
          78376.25732202103,
          53286.6923397533,
          32363.88767512047,
          -13648.765137727267,
          -38247.51514034164,
          17626.67652021556,
          -3905.073440624086,
          65759.57355182158,
          38319.30648067426,
          -7034.736450702553,
          -64467.1423307791,
          -44568.40151458948,
          -87556.9597712165,
          70741.40202114737,
          -1593.0432320119096,
          52182.337139711155,
          -47043.89069439758,
          -73463.0315204713,
          -51215.40397266701,
          -32711.314112146334,
          -20162.26868147228,
          49373.94980860112,
          7181.352955528417,
          -7191.797034544308,
          -78148.08000707101,
          -87603.44531927799,
          -15027.260996177274,
          -70239.61579535525,
          102122.78128743805,
          -24690.97661889187,
          4770.8944445186025,
          11859.181277676062,
          -20247.28013030442,
          8007.069218762821,
          32472.732205440792,
          23549.92587240113,
          11556.480924989866,
          11852.315142870788,
          -92729.45540718317,
          -8676.644004710688,
          64402.33655682299,
          -16393.571350793813,
          24447.483960605405,
          100102.98102922936,
          79448.2531450671,
          42646.40044422686,
          -40003.09369831699,
          -1179.085511147642,
          -53885.99168795876,
          71233.37828100285,
          -64280.71645081784,
          -17117.202445684245,
          6345.032656466719,
          92014.53281519313,
          44973.50161228178,
          -25679.64177317056,
          41843.81534361556,
          25649.33915661246,
          -47233.98677167367,
          -20974.20425493046,
          -4837.206928763715,
          9171.937560225699,
          12895.68441469075,
          106042.99069741114,
          -30368.290017815292,
          4532.807750395755,
          5863.621675453223,
          3333.1469856150225,
          43764.523332405326,
          84151.82859970481,
          -14127.368237139726,
          -2686.474095100557,
          70445.04256164533,
          -71225.79296693076,
          -6759.553625855297,
          49532.39869550856,
          -7130.883088185423,
          -114255.99855530958,
          -118935.55177858329,
          20278.03892453001,
          -69447.495350785,
          -4645.459622916481,
          79269.97470115207,
          27722.582089358144,
          56735.579667141224,
          70219.4762952742,
          -63769.96534617357,
          60235.38273111074,
          114752.48662908607,
          -26619.460025128516,
          29610.75153899277,
          37476.144223666335,
          -44641.00342637438,
          -33107.71228238961,
          -21328.12438507114,
          -60568.60686065453,
          59201.455461523285,
          25075.03103939871,
          -58524.16160868771,
          16880.98615290328,
          20356.489709104877,
          -89687.1270321182,
          9155.325045648962,
          -5552.194626467899,
          -147150.9957530005,
          13238.436433323064,
          5032.732663328299,
          -21904.501011307457,
          22806.91337653757,
          1393.2585804345672,
          55280.926408944695,
          -39280.02791512835,
          30084.483631680207,
          -51413.81027220157,
          -98093.62198557274,
          -45707.575219747545,
          12086.752953463028,
          55252.003841376645,
          109395.00459579841,
          -30897.871261281583,
          -6094.378318214223,
          -3923.3416325510425,
          35369.90753130446,
          -3417.8915618530364,
          2795.5271209084253,
          -63569.07954196342,
          -103638.4321355625,
          82869.34094288977,
          -45944.53968424869,
          30318.260841301642,
          31992.40253373394,
          -49592.47150846509,
          -56785.26015689499,
          -42053.31721878297,
          76894.4872279444,
          38073.79311633709,
          74101.75981177458,
          7043.199045770988,
          -37814.46539723376,
          85221.89789363604,
          -19581.692104471007,
          -66200.19103058847,
          23070.179976279625,
          72682.13545379518,
          35681.644205896046,
          68855.82493967429,
          -2156.6966463329127,
          29597.778262733762,
          -49211.24411674192,
          42065.570302104956,
          -41204.148655954414,
          23725.398393663894,
          -117067.09235519324,
          39072.40865687497,
          -31723.341175422924,
          -16307.645237083361,
          -2558.1045499327124,
          -35153.310179783875,
          33476.59363797042,
          -54747.71094941349,
          59047.565270730935,
          5818.724448488614,
          83500.52734924147,
          -8602.289968575042,
          10723.80855991352,
          -11536.997676556688,
          2663.677171053751,
          58718.818134202214,
          59383.47237437264,
          104952.02521014183,
          -1370.5796527016364,
          -65090.49139563644,
          -100416.38196686457,
          12166.767404543078,
          -45544.460869595205,
          -33830.782997601964,
          66182.38906452968,
          49525.382381082796,
          84233.8943560717,
          -14211.679651127697,
          24996.394512218732,
          126516.98747772988,
          158333.84084224908,
          -11576.667871132247,
          11525.951525095163,
          9502.977629264537,
          -15637.01745352664,
          52292.63918762588,
          53740.14388100302,
          -71534.86096326903,
          98328.84589786771,
          -26677.904893478066,
          -66604.16679040645,
          -53125.223602988655,
          38098.69395491874,
          33347.833826532835,
          54476.16763600769,
          -60727.84860844738,
          122797.88859213167,
          12442.44270497234,
          115773.19994067393,
          -54383.21431400178,
          -101402.4404630406,
          -6096.910020126257,
          -91821.45520173977,
          -63624.42811417365,
          101376.22514437117,
          56815.4193341489,
          -2028.0168283885187,
          96717.37173174083,
          -40710.09699559607,
          70839.11812973596,
          -61971.262996444064,
          68189.18930730577,
          -11311.855666930282,
          -73410.09750584951,
          60880.866038228,
          -7489.0026017749915,
          60815.279865778895,
          -33932.83141722074,
          -55462.826213763496,
          -40708.60398591044,
          -5525.828208197573,
          -5848.003087841337,
          -28285.9303739963,
          13326.910426686054,
          47844.76136136446,
          22674.06251316189,
          65219.73044425478,
          38202.67113463721,
          -158765.19387704213,
          88177.44820858419,
          -8254.332131434297,
          97783.00215495905,
          48018.1473116244,
          -69471.20100908178,
          9414.50431787663,
          42616.609272274465,
          -2046.428882734691,
          -37641.03988263482,
          70752.22686319277,
          -11646.186530025869,
          -42366.04870165155,
          -46174.76259339802,
          -35208.43898677006,
          -41367.99098327047,
          19886.017579223586,
          23596.50352298057,
          45276.839267574614,
          36524.36411630111,
          -9605.817558043856,
          -11844.698436685969,
          50882.67648087186,
          18715.82775350982,
          32592.258136176304,
          12458.44074274674,
          41367.4138307223,
          1588.710455054892,
          -147133.55306192112,
          -83212.0311255201,
          -51069.21793882508,
          43295.01326610508,
          -40431.63306439883,
          68134.59787176478,
          -45489.44021269208,
          -9279.945732798453,
          -51351.2911888868,
          14370.711301612673,
          -3776.1712278369146,
          -77723.71672848964,
          -102961.41751581931,
          -77047.33544151517,
          -28467.514637752443,
          103909.08551716745,
          22863.104371648613,
          -88013.61267749326,
          8184.898439929403,
          7716.159275247315,
          -40463.076237316716,
          45568.744426350815,
          6510.636225655575,
          19227.7874035193,
          -19570.115062454173,
          22439.23724487235,
          -32371.167975213895,
          88410.5282228492,
          87554.08074402997,
          13738.689212618816,
          3564.174862310822,
          -1203.4276150514581,
          -33400.08705736121,
          -51388.46445243325,
          54275.886837778795,
          -58208.036694299684,
          -9675.901825405801,
          -56809.04953552934,
          -985.6638689110772,
          127568.57834460544,
          55010.55949092074,
          39963.731276729595,
          18390.74785621403,
          -19375.039644618744,
          29940.761835302124,
          -92843.0437508263,
          70856.8106045713,
          -34259.334488906745,
          -11184.408352343815,
          65785.6888823764,
          -8293.50397424075,
          33463.7152414653,
          50188.997523035396,
          -60429.046811123815,
          40913.20090974968,
          3389.1916725909255,
          29357.187275892225,
          -11805.905799191683,
          41156.08681666163,
          67105.89147575384,
          6041.162500524931,
          -176141.54443773255,
          -26146.691029451962,
          -41524.675817019575,
          -34391.85291715479,
          -36115.749545042156,
          5172.399159713933,
          33451.61757075225,
          41806.69789824302,
          -43776.585113028275,
          55120.41743311459,
          30187.95994517178,
          -1005.125641906622,
          103223.58008459711,
          62556.96413116313,
          -58307.51449218961,
          -5233.437717333913,
          78282.88325992381,
          58397.627344291526,
          151942.81718734713,
          -27887.300138833212,
          28211.382011965543,
          -62491.59552433254,
          -97087.14643919494,
          -26070.951547222387,
          154907.35193453036,
          -12639.703275978849,
          18281.991622456786,
          -40876.63931016595,
          58754.452299301935,
          64940.664301950776,
          7531.079796581883,
          -37100.32011840353,
          -53008.52397453904,
          -5479.503496241877,
          -93299.75182084645,
          53957.475355237555,
          -137067.93299864282,
          -4473.791888895949,
          -19110.687110313207,
          13059.01108726955,
          42590.54713764293,
          67282.0369119082,
          -63386.29430161252,
          -84131.20607454314,
          57134.70426237174,
          9732.767076960503,
          53008.53353591393,
          -5926.30102411632,
          -56125.62120483471,
          50687.067675934464,
          29223.560528861035,
          -46177.31442446874,
          -30113.729977678115,
          -21317.837006065056,
          68587.29140386604,
          -5682.395939947225,
          156398.00652786656,
          30749.86168764892,
          61926.977469539575,
          -53813.05479361592,
          -111856.36862465908,
          32465.792835978216,
          23685.712645723965,
          12537.384005193406,
          -5727.874848191264,
          33668.9817935633,
          75214.86162107656,
          -57719.5187198118,
          79381.98025749616,
          -58904.29517018401,
          16168.875803727127,
          -25641.717135386203,
          31663.86523896102,
          109976.73861030291,
          -2856.46820085209,
          31386.820479418795,
          35343.61239360713,
          29140.976233684254,
          110405.8920256189,
          51120.970773932844,
          -15768.978689487554,
          -75297.16838117689,
          3832.5528405442797,
          -49649.034893349024,
          141959.2226244647,
          -32400.679185906713,
          41675.60065105473,
          4293.563510619471,
          -17170.03330080053,
          -27612.16996657056,
          30193.239630271994,
          17858.235993586808,
          63796.851444159205,
          -41413.07865788233,
          -86169.49657572761,
          -16121.405170811084,
          -40913.673565870195,
          81889.98605910076,
          -90032.57477707058,
          -54108.67531384993,
          -68169.44093681668,
          40094.564085743,
          22790.921294772004,
          -20363.35927995807,
          -35746.65228531455,
          -75521.22881710489,
          96074.20228461924,
          74041.07748313907,
          -12355.285001635675,
          7028.297887658023,
          -6278.824240180527,
          -56756.89544252676,
          -1422.951452334454,
          -69915.69209911245,
          -2992.4641002797184,
          -114641.76377732315,
          55391.30602495546,
          56086.389992711585,
          -84606.63544329857,
          -46340.257333370275,
          34911.58360745946,
          5761.513104885643,
          -51291.217104029165,
          58924.170158947985,
          42049.85322586653,
          37388.48933134089,
          -76681.4931368561,
          36.7516538408725,
          -43857.99674260946,
          -2903.788931970658,
          90368.56499486379,
          63171.49668843862,
          17576.84990996017,
          10188.643914459017,
          -94147.46650942297,
          -60845.9480864748,
          17702.181766548372,
          -53388.75744223469,
          -44172.2877902955,
          -27298.89599561642,
          9722.88037961241,
          -108006.96415522185,
          -24953.00225968005,
          -27868.002271276233,
          -36791.46500813456,
          233.08410040192373,
          84011.39174444621,
          -52146.77229644662,
          -71698.25604934317,
          -49196.03445745337,
          22898.724706666995,
          -105080.5339932649,
          60600.09720299773,
          -27877.216939275364,
          42482.67801377642,
          -32691.178507128752,
          -13362.21439876869,
          -43264.55965563983,
          4898.647136119371,
          -32750.714953963943,
          28827.68391676763,
          -74129.18979873903,
          2361.3474214801176,
          14575.157642383057,
          -81240.2709841472,
          -43892.19934536004,
          141814.66260079734,
          -24075.02855978794,
          -13301.376421109337,
          2648.3905732345247,
          -23280.308957220168,
          67736.98952037479,
          447.5099833210301,
          -42309.036541300484,
          32409.06595515869,
          -11089.787895097312,
          28055.560660607138,
          -10940.514632942211,
          10748.736252525225,
          127482.38309061207,
          -40285.29767056396,
          1214.4220008309992,
          -4010.246563740356,
          4207.919451242835,
          -41134.86125428994,
          4399.621133503702,
          21219.956956874943,
          3263.684407977256,
          11454.938494775904,
          20864.89259667391,
          -51835.619006304536,
          32675.88507563615,
          -29215.580669210103,
          -113974.96624309711,
          43279.84791031854,
          48180.009285693144,
          49602.61545010192,
          -66799.70785467976,
          65524.795916523064,
          -9595.50022881701,
          38824.581162294904,
          -51447.87095083729,
          -18814.202841178485,
          106253.66376384014,
          102642.43939307779,
          11214.832080100416,
          37784.697660112935,
          64105.113568104505,
          36434.68426912871,
          36914.48447518064,
          -28781.167622603698,
          17967.31663425251,
          -11239.297719214957,
          -61037.60424046715,
          13731.757439939736,
          49233.29258753051,
          -54335.97888802113,
          14249.053260600818,
          100875.91978022121,
          -34657.09792123413,
          90036.56339618703,
          61095.684165784674,
          -28506.781287039754,
          -491.8196601818432,
          -5474.902257865352,
          45483.753963648305,
          -7416.3381138094055,
          30838.444989022464,
          130979.09394107397,
          -10058.53535641986,
          -9930.21675440309,
          -12240.975642206497,
          -13920.805887316077,
          -47198.3539374536,
          50591.716618692495,
          -30213.67174474232,
          48691.00274447514,
          -7961.167030018516,
          -83898.5218939047,
          -39.89410494543406,
          -36899.450264508014,
          -34161.60811363476,
          34734.805412566144,
          -40612.03264752719,
          -79039.18220873413,
          -20389.459719302886,
          -120426.39280955624,
          -8355.665507550744,
          83283.16990889373,
          -17413.876282803856,
          91458.98676896736,
          -76139.26551587504,
          1167.9526139207517,
          -48647.515293680146,
          21610.501117229753,
          15965.068660663459,
          -99043.00039396861,
          62194.54081599004,
          -86601.12031221745,
          -4228.243637820021,
          50445.68900169984,
          -37695.81929424398,
          -4665.711252923173,
          -16559.012556027024,
          -19250.221214127618,
          103703.0111577995,
          11463.042704908914,
          13960.620664709842,
          -9255.841829314433,
          37700.727118532115,
          -39772.74315422549,
          -2475.925701964476,
          45827.19196899936,
          152.07514065658685,
          118524.24936206044,
          19582.143898220053,
          36610.641799206634,
          64317.80801355958,
          99315.7606048833,
          -59958.00742720401,
          66748.25454457036,
          1013.3802447855544,
          -60037.34826424328,
          7079.950683702805,
          -16428.87298488334,
          -36740.01135042016,
          -36603.889440174135,
          90802.99704372195,
          123226.96884078282,
          48606.519211590006,
          -56765.500173714754,
          -6759.382049623581,
          32921.50334573991,
          -57595.98718211536,
          38897.5856775393,
          23125.460517627627,
          4913.105756857833,
          -28550.218640997446,
          54732.180564246744,
          -3763.517156804946,
          46907.729646943415,
          22718.549462965126,
          92104.34482618378,
          71904.17677286598,
          -74583.68366069591,
          43667.9633715382,
          13842.758512335036,
          -81289.06994025657,
          -72740.2325963863,
          -24472.966403245748,
          36130.71421834977,
          67403.44140448794,
          41150.542417107696,
          13850.36640199321,
          -25239.278351790163,
          -44300.03311345734,
          33924.98284681041,
          -105200.19125575093,
          64207.939970208856,
          26268.45597598658,
          -104332.0802592613,
          13034.848907448391,
          85502.47459194848,
          -56171.37359956936,
          52120.856287082075,
          95233.81531887666,
          -18381.285208161764,
          -16088.49521174491,
          -1805.2352672796826,
          -36158.99369574629,
          121122.84572325721,
          -117557.84539016009,
          5402.8662064640175,
          -25650.4586898677,
          -9657.838057902149,
          -374.3075661111739,
          3133.27778545307,
          25083.092314494927,
          14094.500133981232,
          78120.90238911228,
          -29465.213427669478,
          104451.27527427251,
          -74367.32006126943,
          3386.054453211554,
          127036.62627120565,
          -61707.05645976321,
          4027.9950265291372,
          22412.28202178102,
          -27790.72252505603,
          82272.93513791727,
          -5737.679653372398,
          118774.96593126727,
          -6302.151455963624,
          -64571.26477265292,
          21330.144126154068,
          -11360.931728370564,
          11346.122901163251,
          -93332.75117068656,
          6030.2588095087885,
          41182.197881843655,
          -112457.28980368258,
          39436.31504378805,
          58574.663113325725,
          10016.41894718746,
          -5664.094442875591,
          -90192.45355142713,
          12103.925582929278,
          75050.24752255945,
          -1743.127276390901,
          -35523.70146459671,
          82808.99131011883,
          -104638.61589693718,
          34958.901270562135,
          18150.213809427638,
          -73453.3410832135,
          -73688.31822676606,
          -44433.05516205241,
          -69554.62510426492,
          16472.339581189757,
          -89887.00182706356,
          -104628.0399487787,
          -24050.184500605952,
          -40184.33669941791,
          3927.2796854314256,
          -62402.893746632464,
          -66994.9639409953,
          -58383.77239099674,
          -24761.794736978158,
          27440.601454837793,
          40617.58840546743,
          -3943.9282985870473,
          -28561.447309364168,
          -47771.16357547476,
          84119.88800704402,
          -61158.22304582566,
          -78529.37337067623,
          18477.172488281158,
          -1046.4188064664704,
          -43873.202417086606,
          11709.16089620056,
          -17392.007067097413,
          -85421.81765906929,
          180106.06357465332,
          38246.279139713006,
          -13783.909588324694,
          -43368.23476115528,
          -7212.461210272874,
          84454.87143063321,
          2990.2539103398294,
          -4810.802258740176,
          86078.09937617744,
          65025.152755222516,
          78794.46650445189,
          -9375.480332087789,
          -1668.7471219134359,
          112586.03061886085,
          -43424.00877193219,
          -81280.17128732937,
          39233.59952478835,
          -14918.402482373258,
          -9012.188354348431,
          -122257.94945736059,
          14267.248571240014,
          61512.45315705554,
          -29877.141927017317,
          69674.34138141376,
          31792.467774061453,
          8821.458968402638,
          -157199.37506911822,
          -33616.77543317852,
          14388.862441860383,
          -12332.572729613397,
          66343.28046700323,
          23711.168105821012,
          -57309.60707558939,
          -41940.210903324914,
          24084.47605345254,
          18924.27016912743,
          -15976.174101166946,
          -41390.27071610066,
          69401.94741453661,
          -40759.05714824621,
          61360.345171075394,
          111242.2216780487,
          -9134.360523474414,
          -46003.356844028065,
          -54930.566461136616,
          -39989.55552724193,
          -59158.419647326446,
          27113.040782769964,
          18834.61494647212,
          8155.194056287524,
          -9790.233824530394,
          15571.487782737775,
          38778.243825414014,
          -47144.925383228896,
          70570.79912554145,
          -5793.438300231774,
          62493.96941743668,
          115953.78323951454,
          20660.392094448045,
          -6214.58986434194,
          22904.551188378875,
          -63529.95162360518,
          19697.7496837173,
          28046.658313143103,
          8485.348896677115,
          116.76412988325062,
          -40934.123858341736,
          -35381.130759137384,
          -76398.27423874126,
          16174.041544503556,
          78654.78618703531,
          38514.05719385283,
          85417.16034951991,
          38778.54675425268,
          5992.827618330171,
          -42370.29829687696,
          71308.11684121632,
          26454.152233349956,
          29543.446881779444,
          48128.444246262865,
          -29380.417469758708,
          77429.67231374748,
          2894.2058425240634,
          39127.953073068005,
          -15535.211172065188,
          -724.0072790756766,
          -129570.88317042601,
          8560.879276750247,
          -52616.344029769796,
          80443.34501964682,
          46324.35825670517,
          79812.73163517677,
          63254.73888397471,
          13633.989775816466,
          59172.25614386988,
          -72870.16120113456,
          84210.25093376107,
          -12596.135698124523,
          33216.77447806762,
          -42339.45819534408,
          -19281.679228615423,
          27854.3608274143,
          1962.3724675788133,
          -41688.6039067573,
          20516.861789163344,
          -24871.050479165715,
          82449.67697475829,
          -21448.08374161341,
          65706.01285797505,
          -11335.23592373763,
          54392.19083685046,
          -78616.54182790415,
          20578.38734705662,
          -14930.716980797362,
          27767.201021764628,
          -46962.375555308,
          -47535.97988108855,
          4518.9498879557,
          8988.989497956283,
          41337.940850906976,
          -131935.77493521463,
          -75562.19823890264,
          69880.10849436844,
          28330.7239410228,
          -39491.75848556723,
          -44657.92364439537,
          3597.1396024623455,
          43226.21848734941,
          27408.285341042905,
          31110.486931425527,
          -32227.532659155273,
          41095.99290400854,
          11531.346085746532,
          47139.68129796995,
          8233.924426134263,
          5571.961452898281,
          28323.50918392492,
          -25365.960961461926,
          25077.915283274666,
          -59297.85039113925,
          -15449.683665347075,
          -34359.06442645603,
          48022.82994843225,
          -27982.967085184246,
          81601.62846431778,
          14282.477032414561,
          -70722.26230846909,
          71072.30491937972,
          5800.071871086205,
          -40395.19063512269,
          -63433.594179497515,
          28319.444835814895,
          -32046.165445122468,
          -25521.982551681063,
          6214.34814547527,
          -111185.83540150817,
          -21743.07502576136,
          -65335.04498366943,
          -20948.211326133463,
          82104.51182260065,
          -135248.3413212469,
          -31589.987691109345,
          -67597.76960334892,
          -34340.102266729264,
          20044.52429689245,
          19253.897819202048,
          -44103.84675395133,
          12608.625219196865,
          -90133.09185248757,
          35628.33790301029,
          -831.164211880905,
          57283.03648971776,
          -46718.82964629792,
          -93212.45045368698,
          24751.161019828567,
          -46833.546212870184,
          -15700.901478346517,
          -35895.122033389074,
          49138.274929312385,
          -13772.87931772424,
          63475.8306103147,
          -424.5260817157107,
          33662.72408124003,
          -62439.73664362113,
          -94992.548421846,
          25244.473602350237,
          -80554.21919311874,
          53440.53998494158,
          -49484.37091351624,
          -129980.76924031673,
          -715.2443583732583,
          -6600.709811200066,
          -41063.562286428394,
          -62865.788938306934,
          84511.09521342225,
          -114974.20716253889,
          107003.17340701028,
          -31065.252387082866,
          -1365.2921347161084,
          -22269.348441985614,
          123348.70292928895,
          38512.35991537355,
          -153284.85950915093,
          32676.101277688664,
          10243.158270382182,
          9066.565524889178,
          -40855.54276706054,
          -35789.073214643264,
          24012.708448146695,
          40720.91249538607,
          -10310.555670336962,
          -10760.411456765569,
          21333.66121955661,
          57592.62693013959,
          12104.785576899263,
          11226.665398661136,
          34718.85308205206,
          34324.759359315474,
          12486.849827395416,
          -53220.57663760826,
          26232.24982961817,
          51309.44313752224,
          -1722.5109719150596,
          -90805.90353019167,
          -10618.176927130917,
          31886.78676989763,
          27188.22670720336,
          -41610.983863611305,
          -31957.632004879804,
          108882.31095299557,
          29236.86897270443,
          31354.49240971512,
          -22350.008069048963,
          -3844.9947978134614,
          16746.80134827649,
          165568.22278656767,
          -88136.22819418748,
          46378.380715300154,
          17770.428264218823,
          32112.821673510905,
          36309.08496452983,
          -90686.82789806256,
          -133207.2960564287,
          75238.82681760106,
          -69800.0482531485,
          100845.68292984317,
          -23275.00728589392,
          -27035.64131006039,
          -20931.480698097126,
          15061.637764654984,
          44430.43794334525,
          44586.163708131615,
          -80363.28595719139,
          22323.12640910397,
          -4098.6943486661985,
          -65688.31885027779,
          -79188.86669095338,
          76826.88158779006,
          -98530.98781657139,
          -60652.46412138893,
          74818.08288458471,
          -25009.540636310667,
          55057.745980465006,
          53533.61238884356,
          37918.459611955266,
          -17887.70168844752,
          56339.178519613946,
          32996.41722997635,
          53511.7610033749,
          28150.3563748276,
          -26796.23746696657,
          33464.75754446963,
          46576.2691257761,
          29143.23784891843,
          7299.166371027889,
          72054.66443395511,
          51637.197294340374,
          62436.280924866405,
          1562.666318295677,
          -123210.19078856982,
          -59202.20834058208,
          -63697.30314872977,
          -13514.36190324033,
          86825.61460103809,
          -87209.52529420302,
          27101.43611286692,
          -19179.03612134546,
          -82634.7524850452,
          -50564.58824574469,
          -24981.74315206656,
          37876.63000420639,
          105177.36925358517,
          34194.055581849876,
          -116587.59670126768,
          -14169.264058411283,
          -73316.61412294635,
          50411.0611893681,
          -8065.290111516544,
          -10157.337595139874,
          43185.04570195617,
          -2610.4831655867347,
          -118992.47387328402,
          102318.33610389373,
          19427.69300580493,
          -55817.040454177884,
          15185.320273590918,
          -13071.84618456924,
          -66167.62790622888,
          22464.711674248825,
          -83140.17089680475,
          35547.51549591102,
          15015.001177929042,
          -16967.543167936743,
          -77369.73262936999,
          -105608.92032118682,
          -17634.621133314206,
          38335.97371210768,
          72917.27160358934,
          36375.679135078535,
          -13415.246258943005,
          -116276.27134189695,
          -45325.253828090375,
          34078.07777388741,
          -36111.57241281199,
          -98895.6711206625,
          -70805.09024999585,
          106715.83646642428,
          -40748.08128099842,
          6241.076025478806,
          28867.732029816998,
          56130.51153200202,
          3152.711003290252,
          -93343.82989537754,
          -8004.517094451758,
          4462.801026137343,
          7496.898123834987,
          52563.76836671455,
          -7982.821861462198,
          -36392.228910400416,
          -63652.41449187842,
          740.1837050969169,
          37684.94218505975,
          -5604.342109268536,
          -15461.040356453857,
          -63149.47427869277,
          -42825.4133506964,
          -26483.947464805886,
          111601.61375898316,
          -20378.366478153017,
          30636.190690361564,
          18833.507823320026,
          80410.69774996408,
          22131.647678013233,
          -71423.01832480013,
          -18065.267012237506,
          30622.329294829353,
          129008.72505112807,
          127762.05967825507,
          16914.848085784346,
          -14362.491216284114,
          -22728.340543059123,
          11942.171403520515,
          82792.92275993899,
          22323.603741039427,
          -130749.36832771148,
          -78323.73334333953,
          -23262.053588371262,
          -15731.193267603581,
          51652.57360629744,
          -63205.68656210791,
          70832.41457464529,
          -3625.281816070314,
          32103.355025546258,
          -3847.6507675613097,
          -89897.45845432584,
          -18218.929713413825,
          -20420.822906510522,
          -9404.510853832488,
          113322.88477242591,
          12665.029399758403,
          -96279.27355934448,
          58896.09774187914,
          -79400.41624201072,
          76699.61615471804,
          104963.8817861476,
          -27770.40540697716,
          16398.337494461368,
          -80827.18979548413,
          14223.466729775546,
          66438.23743273504,
          -94231.85949947443,
          -21799.073601019034,
          52955.53082282577,
          98760.4843582624,
          34496.44963082951,
          -70661.45910128426,
          -57415.81123220784,
          -80786.29746925157,
          -15000.885511194658,
          -5683.066019203408,
          -15517.7828361074,
          -59667.385860207396,
          -29415.185713231844,
          6827.001159249797,
          -41485.12845375917,
          -56211.942583471304,
          30364.941738276153,
          53592.12068053393,
          22885.319429265408,
          -10258.040290494258,
          -18835.317870198076,
          -59976.78346299702,
          37859.50038041726,
          -8605.804274140866,
          17549.531686781862,
          -36552.17182116446,
          81977.97374068329,
          -27254.70548101355,
          -2445.256961454014,
          -95086.32754639235,
          -81062.63982343081,
          -15604.674645275187,
          6850.912120819427,
          -92760.40847834299,
          15071.264492857235,
          1667.2290332852772,
          29222.667895062412,
          21284.999613263233,
          -32311.10620399896,
          -57237.84534704073,
          68395.66804339505,
          71198.98920487509,
          -11707.96513567088,
          -105563.28565361029,
          100264.78555739798,
          -65901.81984477652,
          122338.52082570909,
          12421.122452676074,
          -33073.834885381366,
          26812.30528867826,
          52993.34506515217,
          -43950.47490625822,
          53158.32250605196,
          1351.2576073196517,
          90189.27368597452,
          44117.46922011644,
          51665.491860415306,
          37422.73207834763,
          83908.80162201676,
          -48568.56686011634,
          -28317.712742204396,
          -17086.80670630984,
          55291.66579168892,
          17932.83779621348,
          51475.82527111193,
          -1076.5067543856112,
          -11730.710355972431,
          -71022.17288091771,
          69568.11056876984,
          144185.26172787446,
          92538.52482539321,
          -106668.23131652885,
          32138.01598304438,
          101785.34224183772,
          25434.659145087437,
          44485.124500823425,
          64023.38606157139,
          -66017.90288131642,
          -36116.02102862788,
          -85730.31447718687,
          -18446.450271422877,
          81463.33918939886,
          27794.026255837623,
          -72443.45170674208,
          -10753.135387007516,
          22245.878712006102,
          -53471.792081838204,
          -74725.04977466146,
          -42508.33280162503,
          40690.403386297075,
          -21905.159037938127,
          88266.34091642119,
          -19796.007932675515,
          -21118.64939399878,
          -273.25234026802525,
          90581.87298589073,
          48511.667604067814,
          36611.8633338053,
          34459.93180810433,
          -7164.05263987093,
          26481.764198966943,
          947.4376847945787,
          15002.237882130741,
          -29813.84195148732,
          88223.0413182867,
          -22294.63171580312,
          58435.16885983356,
          74593.51410739229,
          33187.14321502791,
          -23874.81989324627,
          -34950.4690667114,
          48149.817544313206,
          -39092.52760457902,
          3631.8913322735566,
          20150.50231693677,
          34031.35812330896,
          -39535.526634538466,
          -8315.324396405062,
          -17672.272668002686,
          -13375.661323329796,
          48998.98728379024,
          -36893.32646203928,
          -94165.69758733788,
          -15183.417240136148,
          9975.888378124288,
          40071.61807316738,
          -32752.75993524292,
          18249.442040064234,
          7891.771116690464,
          14880.151779679034,
          -80624.50189603755,
          -76204.92968136676,
          -53705.35418554207,
          -74014.77665195761,
          -25016.75304466701,
          -15557.37551923828,
          24720.82875766355,
          -61901.95309404655,
          105247.36217493453,
          67353.32922399437,
          -115624.59486649962,
          1989.1507455531168,
          67009.4846973737,
          -59679.90463545195,
          10197.858332701999,
          -38388.72189697463,
          88811.46006856882,
          14578.263855515383,
          -17726.80537625451,
          -156606.66395917433,
          76331.41379816,
          2545.631376665087,
          -81240.00560469685,
          -57299.43912533336,
          108044.97219714861,
          13363.13946832634,
          25961.172466837474,
          -14337.448830796502,
          44317.69571252549,
          25760.71638472731,
          65296.56845631454,
          913.1999689158395,
          139228.9948481203,
          -73274.25929429896,
          -9140.473888918945,
          104065.34366157683,
          -115852.22747557377,
          107359.68520103778,
          -34613.39527552246,
          1679.5956277417386,
          31755.91860763817,
          -96139.09080641207,
          -34506.26219184342,
          -75748.39897786145,
          -8329.543364536656,
          62173.83313578053,
          -14765.437721101269,
          -31015.704784260117,
          68122.97681829274,
          -30513.776794052395,
          139323.64754820368,
          84030.53302059912,
          47343.41577555402,
          -17515.841976658976,
          37926.09768193201,
          22443.960938011856,
          -17681.18867442055,
          27658.602071268593,
          -77811.57587290254,
          -67351.37635320524,
          56260.28941654608,
          29524.271494966677,
          -98015.97046995939,
          46100.7223172865,
          -32315.329716140634,
          -20999.614923376368,
          -74114.19389763601,
          149233.87723076926,
          -42722.98830100669,
          34134.30926687475,
          -14691.71619409911,
          -40477.94150418921,
          14851.16715962938,
          13802.73523459667,
          8322.93448109704,
          39978.789278719974,
          49076.69406657819,
          -53395.77206496188,
          80378.73728889303,
          92757.75857037824,
          -30813.991023150556,
          51669.16880457816,
          43972.78463475775,
          -52674.21889990014,
          -68547.85130783399,
          -8737.30734802702,
          22419.7759507407,
          42170.284266571696,
          39594.934163168815,
          14610.675587407137,
          4397.529650561397,
          55165.439384757556,
          -81257.6362673351,
          -116740.33698832875,
          21491.754128078363,
          -77591.25770270864,
          35632.50567688474,
          24128.249121825622,
          24371.189235717116,
          96572.1340893621,
          9117.143028818056,
          14657.112244693915,
          -55868.960018672326,
          -16188.008167207656,
          -16327.21901391155,
          37531.69463177802,
          58135.93978404826,
          -95446.24522918483,
          -38561.6560531964,
          20187.994321485203,
          -36086.01594086868,
          27511.806871356806,
          28259.30088682302,
          29583.717098404788,
          4450.805800556692,
          -64643.545871412825,
          3797.7524849096744,
          -66614.34132330463,
          -23469.506634589627,
          5145.671039675591,
          -18063.929454530957,
          -35460.255497836144,
          18222.176145179277,
          10793.214367297915,
          -74569.30060019306,
          10669.36023000264,
          -36531.480490252485,
          -68050.76910757899,
          484.4702881626716,
          -34699.541752646306,
          66047.50890549849,
          30149.62639045158,
          -4446.265643444219,
          38909.509546432506,
          11042.110256963932,
          6803.669498270227,
          -69131.01962103481,
          43657.317180557206,
          -51099.259968116836,
          52220.22270821699,
          51796.325866694635,
          5996.041814518699,
          91925.10377370822,
          -11526.517230570076,
          -5825.335814556036,
          34848.55845179472,
          -39996.5627488698,
          -58901.91156124234,
          -3004.7058437641585,
          -42176.27241351394,
          116777.05782263995,
          -22278.15563653858,
          23299.30315247037,
          -59432.729777333894,
          -12764.447299224079,
          -5119.17619681727,
          -61602.7314809633,
          29632.136469651883,
          31288.461989000258,
          -29141.82479438826,
          74226.37844941797,
          -99653.21322412638,
          121234.36120277591,
          -18800.244044919586,
          -79934.55611965837,
          -37363.071255631505,
          24799.23154564725,
          41666.87900328312,
          103397.34976998808,
          -136.83754670436554,
          87970.24663172856,
          -33686.60950159953,
          116464.8998772848,
          -87024.6650308293,
          -72415.50554238846,
          42889.192295141445,
          -53053.045928001186,
          16725.079679849954,
          -52615.142224440235,
          52488.47959545366,
          36712.83029745912,
          -40027.62662901472,
          16577.524175581173,
          67324.33763470185,
          -37763.33809399563,
          15872.352291007974,
          -21262.715450818214,
          -4020.5687114615066,
          14957.22980283272,
          143724.71877326866,
          89795.10485783756,
          -5495.458275936076,
          -30810.128365566474,
          -12092.819444395804,
          -79383.40296661311,
          11348.629021836281,
          -2218.5676511837255,
          -30305.92977267115,
          -39683.03669903483,
          -46257.266536609095,
          -22812.705228996365,
          -36218.803649265355,
          -29702.762286808636,
          -68304.68316043398,
          67058.0826773579,
          61097.650234220935,
          -3212.440724871636,
          -54914.9815482668,
          -88447.53574827281,
          8167.601833660371,
          -52117.12820265766,
          -106401.30116483294,
          39739.333395610534,
          44434.76838746586,
          40810.50075281989,
          14506.78218838684,
          96608.50717364937,
          59714.80432974491,
          -41972.38868561229,
          59702.34731620012,
          37088.51467627483,
          -87659.02474938505,
          -70360.7107182604,
          9078.58859738938,
          -72999.9437139,
          40223.970883340655,
          -25504.38831987589,
          -59808.09201262706,
          1482.954587809934,
          34870.134715095075,
          13619.255423051027,
          105673.63131245297,
          74246.87524491982,
          16465.740952949604,
          7235.680180817732,
          -56305.864807902086,
          -18524.699118196404,
          -2968.4537902263955,
          52997.848298273006,
          58708.52479766778,
          6318.879672006694,
          55074.22213693784,
          43199.14268739396,
          45926.70501818612,
          -18811.510282611045,
          103890.23526059688,
          67941.79891395292,
          -77394.3518319389,
          -19731.50646341882,
          57373.81343950599,
          -66242.85648014997,
          73219.85303948869,
          -2556.3619537450545,
          -22462.902872979244,
          -30431.560079695344,
          117321.16312506581,
          48887.11656988668,
          -116711.40102686222,
          9190.61401588278,
          91131.50473520262,
          104057.72356746296,
          -22559.57033205525,
          29092.790447043244,
          -16198.051007489317,
          25847.351539613857,
          15616.203371750093,
          151960.67067099546,
          -61802.37747454607,
          -34558.08974774185,
          18997.10985143377,
          -48356.616140346116,
          10048.073742634027,
          -55957.59667823219,
          8172.932737045337,
          484.92644353770646,
          -84304.40383389567,
          -10894.722693502337,
          -48178.13541483882,
          49155.09888626897,
          -51118.7080998406,
          119462.65569047391,
          24754.092969832367,
          -33619.48795881496,
          -88064.28243647201,
          103917.14580144089,
          7314.728878233258,
          85840.7918044661,
          118159.69731855494,
          8819.447205401131,
          40971.923598970294,
          -3539.3721042685047,
          43432.051565269416,
          -21515.585906305874,
          101987.05550657456,
          58064.58419276893,
          35278.703821135045,
          59982.438531678315,
          -47180.93191618678,
          -8648.60881847097,
          25341.17503518477,
          -36212.81555002148,
          -70683.77806549122,
          -62218.09064036916,
          -39504.14713098946,
          -93087.08371377604,
          13044.609176341826,
          8168.44021088258,
          37261.94614303128,
          -4824.268995928516,
          68448.31393358817,
          -14786.409661844695,
          -55607.17102354964,
          -38924.440263016644,
          -50828.52031527281,
          -46650.64828053422,
          -16428.173366531173,
          -29079.46892128171,
          -33112.99843989561,
          -47276.95274883654,
          53416.077934135516,
          -102168.06673392371,
          18894.464126816627,
          18933.848121759525,
          -34317.75227445398,
          58390.028459773086,
          -66569.35578586592,
          -72632.23823649055,
          -45581.83285611196,
          45978.84339460768,
          91212.0484600298,
          -37258.70643987752,
          19545.660001596996,
          -41061.86132143274,
          -57446.93287179173,
          16854.389964942005,
          -123049.19516054474,
          12642.546477714006,
          -32247.087612759955,
          140465.7668307121,
          -83087.2580248384,
          -49855.632177260195,
          -67980.42947721513,
          -66194.38252064805,
          83749.07416433138,
          -113516.00672099482,
          43584.19738825062,
          -13483.39844348418,
          -19911.456833567834,
          -35534.89590486394,
          27655.315106626356,
          -47678.823365436896,
          63124.569957433436,
          -33510.3231697526,
          103877.51694562614,
          93599.5976404022,
          -23556.422541792985,
          -30157.63321385247,
          -54735.80131909644,
          75928.77724178764,
          3644.793577238804,
          -65185.16851107277,
          12489.63170411699,
          -108376.18457565214,
          10119.61642295754,
          -119555.70041614107,
          -51075.06772017414,
          22849.01315701065,
          -34824.55120938794,
          112534.5825388806,
          29012.38468483206,
          36765.32631139743,
          50888.86839275412,
          66124.09410098159,
          71294.69307630367,
          -63230.09198730894,
          -43869.093794950815,
          18772.841063351538,
          16686.75555228699,
          -139543.75194507293,
          -96715.44412701167,
          -49057.304500419916,
          -108261.01960894847,
          33692.64269052327,
          36860.66822073754,
          61105.74801877028,
          57007.031062796406,
          23633.561188319673,
          71646.66184776317,
          89162.76531924064,
          -39223.90568372043,
          -40161.108585256494,
          -38483.552087323194,
          -94147.38883262115,
          -66172.3424765576,
          -153525.04531782304,
          -16681.93409978936,
          32657.39472760564,
          97282.64354026166,
          18481.556300041037,
          30759.91336939226,
          36204.63861653253,
          -65104.536347307396,
          -6442.020378471836,
          -27372.877780197716,
          18071.398846882763,
          -93835.19352404664,
          40535.79470204725,
          -15338.385441347094,
          -41699.802495226606,
          26992.262004675344,
          -13723.791754031907,
          -43340.213468712944,
          -47286.906207033026,
          4612.118112261113,
          102221.69376590101,
          -10816.293791382774,
          -2500.2901040668416,
          41117.35444836173,
          -72076.7490091258,
          -55449.37599256977,
          49108.00109554557,
          31043.6099666245,
          22115.647748048792,
          -12616.921849926059,
          -72328.42531381376,
          -55223.76127047198,
          -92396.79188840456,
          -122792.17338155243,
          11689.922475060475,
          65660.26363813395,
          18822.07226490282,
          -8291.642941066531,
          -80241.23025949216,
          -2652.7678119018515,
          8440.986103739417,
          31713.275447057156,
          -34395.12312469852,
          68013.72649696992,
          66272.3964977179,
          -76497.79646596457,
          -4954.616835767943,
          11723.414425597479,
          -36136.010766906234,
          -38707.58543100731,
          -80004.58799862617,
          13405.185053799481,
          -8691.58090609947,
          -17226.465618524137,
          -64466.73953594784,
          -41960.271280342866,
          -45753.2607335997,
          36399.51091390568,
          85382.71523131379,
          52982.61611248947,
          -33558.17411793224,
          -72224.41165071886,
          124277.91590389381,
          28784.62134369536,
          -87600.6127946429,
          -74562.80509871259,
          -72128.59362033424,
          -1448.1490667805524,
          -33510.11637243212,
          77662.83201130704,
          -47581.417085107605,
          121706.30310798076,
          -10185.049529502328,
          -33196.269135616174,
          -55530.48130877368,
          -30987.75838651185,
          -28049.077609709308,
          42861.93064542104,
          115403.61793714094,
          23262.14781862036,
          -65277.87469701173,
          -7406.491823754026,
          83448.42016475841,
          102450.8541055578,
          -32319.592488031558,
          -42325.06763675415,
          -25714.49924223357,
          -69785.77945402577,
          -75339.98375957932,
          53867.59955610693,
          90072.02924970683,
          -29093.8402221576,
          -77711.14506210003,
          84212.03700572642,
          74340.58195554647,
          -41171.24422317322,
          -25192.162142335208,
          -4027.1544699529195,
          135970.2622457784,
          5447.379972204467,
          71159.09809144097,
          -2110.7893390877525,
          -85297.51618729845,
          -47532.175047661534,
          -32727.875943096813,
          -39859.722592711296,
          -74001.41734172724,
          -17431.231422608624,
          -52781.57272469347,
          6110.977374478352,
          71379.50150253517,
          62401.0109690598,
          -88545.96996653076,
          9457.306245863276,
          -18820.99287516403,
          12266.363498731998,
          82899.5496672509,
          -13423.28732165291,
          -75774.78125350519,
          -101844.71599221756,
          -59554.1278698473,
          -16211.313549021357,
          49527.47607176042,
          65026.365104322416,
          -12227.62160171286,
          -19275.458399203897,
          9489.329530187602,
          -6363.62880613561,
          62086.51530179166,
          56590.944547107574,
          10656.048831866336,
          -10963.15914235067,
          4713.4634113862,
          10441.1038792095,
          58713.5870837675,
          -36135.63566784937,
          99075.48709962053,
          32169.711004068777,
          -8595.247807506712,
          80193.83689375935,
          85186.27559180658,
          19034.694117939183,
          -12285.252140995664,
          -15055.378069212748,
          -51543.06213564062,
          -11856.965724482468,
          14852.99092077441,
          11387.321997445912,
          50482.16740619361,
          -40223.077111564984,
          8436.63232089593,
          -111324.13399614423,
          -33258.596161511385,
          -9819.441429020413,
          2656.1487657514995,
          77762.41986201196,
          -137995.0633404951,
          43431.92057887121,
          -70292.86293006518,
          -2869.9123220955103,
          -20503.099352263263,
          -87988.19469297667,
          17731.6015895732,
          74242.09577026524,
          -65366.98089490949,
          -134195.27613729396,
          99543.01127729534,
          31665.954306978565,
          -22515.018350341397,
          -27949.999509510853,
          -27165.394112873324,
          3132.6747223815564,
          -48941.62471346166,
          10548.628325924916,
          47096.96569440038,
          -48784.900501597236,
          -31630.21831592611,
          -130819.00380227089,
          67505.32401344182,
          70029.64350813408,
          15301.41954306318,
          40809.64476670628,
          53167.13835072176,
          -130403.00371633655,
          -16625.521252839073,
          -161284.10735563678,
          -56604.2629588539,
          -87798.90344868311,
          13042.169510210859,
          39794.479977440045,
          -41038.22587327816,
          9536.474839813332,
          -36701.67574902413,
          -41923.74890015072,
          130377.25258442962,
          -119521.85117206248,
          23258.676481891624,
          -31258.323245215877,
          -13510.601110287958,
          195811.17775836348,
          79372.27020178316,
          -57266.59309788794,
          19799.07400788295,
          -65463.73691366915,
          -49079.90983747176,
          75043.3987094801,
          -14532.079405061037,
          74524.67552346499,
          84158.15895628088,
          109374.78197962683,
          -99932.2796352183,
          37987.887069823795,
          -33728.740109546314,
          -46658.61511134989,
          33829.6252427846,
          -52925.67672426903,
          -844.4298367425927,
          37917.72119987783,
          19637.90760311299,
          158636.16224527315,
          -10055.393403590848,
          90409.98912915195,
          -35681.82012689742,
          -102165.46264124088,
          46291.01210205927,
          -15563.725061814132,
          -114156.69576980396,
          44607.91382605125,
          -15034.032623033441,
          -116827.891615484,
          -9149.591573549129,
          39435.87244375716,
          26409.4793513756,
          -63081.86027868048,
          94959.07848241484,
          -42052.003368979545,
          -109259.39208787438,
          16307.818249033513,
          -13489.224328219078,
          -29033.910416379877,
          -73153.01451977277,
          -80557.95806616853,
          33328.7913813127,
          150642.32427219045,
          12003.95825482041,
          86616.34171550845,
          78772.21108372913,
          52152.324299240965,
          20559.193707861832,
          -31220.200448363394,
          35092.83583608723,
          74841.69615719805,
          -83127.09944577022,
          107746.60340294881,
          -109375.43341545745,
          -10985.976850423185,
          -82580.46342223576,
          -27490.618472577702,
          -1583.6021966031979,
          549.4538036947018,
          10985.037918517124,
          43603.2811572513,
          -152538.0717275921,
          26030.202289987188,
          -13736.284152359603,
          -72443.6308059312,
          47001.57667097059,
          -82057.50039720266,
          16758.36631940808,
          15675.876848848102,
          244921.4999545079,
          65876.10725653598,
          -51635.27178285713,
          -22370.097551891224,
          -25988.161835703937,
          -45057.39153812341,
          -44964.589277365565,
          52998.081381077085,
          38780.190356263636,
          43320.94529837355,
          -3371.3806317684553,
          -21085.228469428825,
          87760.73205215718,
          -60657.34117226955,
          -41679.935523550535,
          -67561.50320417824,
          48480.48744890539,
          43453.62914860345,
          -52175.34768130357,
          -2969.8416821951,
          150534.80042512846,
          107915.25386989731,
          -50699.82868700624,
          -54749.89258862078,
          -102841.36153550606,
          803.4232660634968,
          -44087.52888334742,
          10301.487565347292,
          -92971.46306408566,
          40047.58890014892,
          -50045.38838190133,
          -72028.5585493306,
          -20443.606080297217,
          75847.86075697145,
          -50243.116896291256,
          63342.06006296482,
          47131.768425301896,
          78450.10536941253,
          -11239.926853807294,
          -35230.791824904,
          -48109.53718719937,
          376.2232982557228,
          77064.67522158404,
          -10914.831531027266,
          -136240.55586287202,
          33046.60864658149,
          -64118.237061943044,
          10887.126136432036,
          38448.29118412735,
          36257.32415038368,
          5810.839642797018,
          19284.02041361237,
          9224.698982140355,
          -37172.06489565202,
          34574.05857123271,
          -9653.762366196475,
          11470.826864031185,
          181287.18661954743,
          6023.230701869009,
          57957.02844305192,
          -83164.20080602553,
          70476.59955406938,
          -47213.39276735441,
          13392.750821016632,
          33748.14590835511,
          12242.160587034017,
          -11413.287134673292,
          -89954.11060342516,
          71800.09714785397,
          -25797.68890408457,
          -62376.907388011954,
          -97429.65919422465,
          -3861.583343641475,
          -46276.738091020015,
          -32274.020356955025,
          35872.156302811,
          -76130.71461965598,
          -39038.417165567116,
          -83695.48144304744,
          -69062.56373757735,
          -37323.35503531719,
          -18781.10130307859,
          -39945.95297290979,
          -63699.388028468755,
          -4098.995841321524,
          -24758.80570727766,
          -105000.56827520095,
          -5913.014573319399,
          43225.48281357793,
          -4817.376828932352,
          12647.005193672743,
          28890.345061504515,
          4271.509972419703,
          -45304.1426542693,
          73543.41931628819,
          24491.06850049533,
          -122696.89679804396,
          -85698.07495650547,
          -29604.753962636405,
          -90406.66671307375,
          29166.429285290025,
          82.33313951895572,
          26997.459867777005,
          41322.33568579236,
          -5110.071955973993,
          -25506.571100441426,
          -53098.71932420179,
          39291.02580087217,
          -73874.38972455826,
          -117691.79394322899,
          -19562.96418899556,
          -82076.21320542628,
          -62157.69981191284,
          -93837.11513381032,
          14710.387507156544,
          -9427.752988049266,
          -93859.01520035307,
          40816.57116408857,
          10719.885304075813,
          29211.598834550845,
          15610.397503229864,
          32869.0050864381,
          -38458.16110587183,
          18996.624098709475,
          -84266.4241688168,
          -75798.5781148906,
          28626.190441423965,
          59944.060718355424,
          -13688.509960283262,
          21332.81893549343,
          -86245.54681685162,
          -28393.364623641137,
          -32447.541608218817,
          97235.16170446927,
          -136114.5583764514,
          -52659.084675978644,
          -69414.05639659424,
          -2710.6132682319076,
          66645.13968674676,
          -39751.9299373475,
          -24236.359970496447,
          22566.477773907194,
          -46027.92933910719,
          10398.14538108743,
          43909.1992337507,
          70430.79817475776,
          -50022.30607245092,
          8770.1200808914,
          36308.15201799585,
          -47355.90400317422,
          -25425.880056052356,
          40391.78659053562,
          30811.36753794,
          50517.723790455486,
          -43358.07611540121,
          61596.665486577556,
          81965.31503353742,
          -71468.04986835172,
          -10899.821218594558,
          91757.79408572361,
          -41833.58068501348,
          -60921.82291397684,
          75417.41400759616,
          -108963.61902006746,
          67652.98626862535,
          89466.62709418079,
          38302.08912936318,
          -95581.09213145298,
          92075.13335188862,
          -52683.15133790177,
          8738.884542859596,
          1416.554141807525,
          7821.702865940975,
          12786.16116835164,
          -57147.02001335979,
          1526.6038310121455,
          3270.3684189807864,
          -36942.993636886284,
          99988.58615099775,
          -9172.418520798965,
          108333.2284942167,
          32340.243596676228,
          37309.54257341361,
          -39106.50027259727,
          -8716.383251322097,
          26298.50950205415,
          -10367.629424722953,
          40187.55764951773,
          79335.90632014778,
          67804.3989461086,
          -122411.47088840125,
          -23604.646697323347,
          26265.053053871583,
          35923.7489668846,
          -28097.899480036227,
          -35899.40780448945,
          -93608.18339938378,
          -55533.330175523515,
          23770.739383829405,
          62809.62810026866,
          58297.59897032432,
          -25416.958185787644,
          86202.0732244695,
          110705.07567518218,
          -61438.14928341999,
          -21097.49168616028,
          -498.37137955289353,
          153065.06270877528,
          109449.59885431435,
          -61845.45780928526,
          11067.81233433484,
          -67458.8534461739,
          -56104.961785178595,
          91153.36415925283,
          -144588.30467086937,
          52220.28780264072,
          -42600.16320467139,
          28334.781264033816,
          -22579.733380118516,
          -88594.89392955555,
          34589.9091636633,
          -60839.66924017053,
          -81740.29460788256,
          5340.32506731622,
          -112096.84908354474,
          31290.988306001207,
          -37028.71687368472,
          82725.12932299815,
          41398.47659192522,
          -23629.890314600605,
          -705.6664181377656,
          -4837.611790020296,
          -61472.714264046226,
          114721.03327960259,
          19743.3830856365,
          -79400.01938430272,
          28927.897321130222,
          -91096.84888333957,
          46528.9679664203,
          2875.2319798022145,
          24636.535153578003,
          -83463.05199843177,
          51644.88825056283,
          -38637.934925279056,
          91535.82349111978,
          -44939.598844271975,
          38897.70535200195,
          25400.331450334448,
          -86636.91119540784,
          24360.616036091607,
          25925.987810697385,
          79641.8390832598,
          -28625.690332995804,
          55160.96066304114,
          5756.365514770951,
          76889.72472656996,
          37389.39503344396,
          -9596.516667950988,
          -108380.34426864183,
          -73247.05566485875,
          71158.7345432589,
          59130.86507862475,
          -139399.9536431551,
          49100.61427054639,
          -37408.420889606576,
          -22288.638830566615,
          78444.0690396556,
          108367.02503466715,
          37957.331291738505,
          48040.044237560985,
          40955.4997563561,
          -63859.85010127018,
          -65765.53731115544,
          -37921.23552670279,
          -31030.047739881837,
          -46702.67571071557,
          -30739.15601157417,
          -21623.83678223346,
          44877.58987288573,
          -39469.97364345158,
          -82369.1308762908,
          -60723.94550543029,
          62168.39655257167,
          13765.35389050557,
          -27404.385270501534,
          54402.14973274412,
          28653.657456473014,
          9311.19839187211,
          20591.274763711,
          -55374.00330522045,
          -108204.89298713337,
          -77505.07480160371,
          54240.679421722576,
          -41973.8976220926,
          6283.341998082033,
          -70010.790763359,
          -52276.305642059575,
          -2417.496379302828,
          74543.23623321357,
          60780.52417050375,
          -36797.956329069,
          8495.185679548698,
          50268.010641241,
          -74623.55097488142,
          17604.34272527484,
          64980.686675129706,
          -9871.916160469364,
          2443.7606813821385,
          40321.24001954007,
          11374.561212703404,
          -26324.64419753878,
          -36674.4678593847
         ],
         "xaxis": "x",
         "y": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999,
          1000,
          1001,
          1002,
          1003,
          1004,
          1005,
          1006,
          1007,
          1008,
          1009,
          1010,
          1011,
          1012,
          1013,
          1014,
          1015,
          1016,
          1017,
          1018,
          1019,
          1020,
          1021,
          1022,
          1023,
          1024,
          1025,
          1026,
          1027,
          1028,
          1029,
          1030,
          1031,
          1032,
          1033,
          1034,
          1035,
          1036,
          1037,
          1038,
          1039,
          1040,
          1041,
          1042,
          1043,
          1044,
          1045,
          1046,
          1047,
          1048,
          1049,
          1050,
          1051,
          1052,
          1053,
          1054,
          1055,
          1056,
          1057,
          1058,
          1059,
          1060,
          1061,
          1062,
          1063,
          1064,
          1065,
          1066,
          1067,
          1068,
          1069,
          1070,
          1071,
          1072,
          1073,
          1074,
          1075,
          1076,
          1077,
          1078,
          1079,
          1080,
          1081,
          1082,
          1083,
          1084,
          1085,
          1086,
          1087,
          1088,
          1089,
          1090,
          1091,
          1092,
          1093,
          1094,
          1095,
          1096,
          1097,
          1098,
          1099,
          1100,
          1101,
          1102,
          1103,
          1104,
          1105,
          1106,
          1107,
          1108,
          1109,
          1110,
          1111,
          1112,
          1113,
          1114,
          1115,
          1116,
          1117,
          1118,
          1119,
          1120,
          1121,
          1122,
          1123,
          1124,
          1125,
          1126,
          1127,
          1128,
          1129,
          1130,
          1131,
          1132,
          1133,
          1134,
          1135,
          1136,
          1137,
          1138,
          1139,
          1140,
          1141,
          1142,
          1143,
          1144,
          1145,
          1146,
          1147,
          1148,
          1149,
          1150,
          1151,
          1152,
          1153,
          1154,
          1155,
          1156,
          1157,
          1158,
          1159,
          1160,
          1161,
          1162,
          1163,
          1164,
          1165,
          1166,
          1167,
          1168,
          1169,
          1170,
          1171,
          1172,
          1173,
          1174,
          1175,
          1176,
          1177,
          1178,
          1179,
          1180,
          1181,
          1182,
          1183,
          1184,
          1185,
          1186,
          1187,
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194,
          1195,
          1196,
          1197,
          1198,
          1199,
          1200,
          1201,
          1202,
          1203,
          1204,
          1205,
          1206,
          1207,
          1208,
          1209,
          1210,
          1211,
          1212,
          1213,
          1214,
          1215,
          1216,
          1217,
          1218,
          1219,
          1220,
          1221,
          1222,
          1223,
          1224,
          1225,
          1226,
          1227,
          1228,
          1229,
          1230,
          1231,
          1232,
          1233,
          1234,
          1235,
          1236,
          1237,
          1238,
          1239,
          1240,
          1241,
          1242,
          1243,
          1244,
          1245,
          1246,
          1247,
          1248,
          1249,
          1250,
          1251,
          1252,
          1253,
          1254,
          1255,
          1256,
          1257,
          1258,
          1259,
          1260,
          1261,
          1262,
          1263,
          1264,
          1265,
          1266,
          1267,
          1268,
          1269,
          1270,
          1271,
          1272,
          1273,
          1274,
          1275,
          1276,
          1277,
          1278,
          1279,
          1280,
          1281,
          1282,
          1283,
          1284,
          1285,
          1286,
          1287,
          1288,
          1289,
          1290,
          1291,
          1292,
          1293,
          1294,
          1295,
          1296,
          1297,
          1298,
          1299,
          1300,
          1301,
          1302,
          1303,
          1304,
          1305,
          1306,
          1307,
          1308,
          1309,
          1310,
          1311,
          1312,
          1313,
          1314,
          1315,
          1316,
          1317,
          1318,
          1319,
          1320,
          1321,
          1322,
          1323,
          1324,
          1325,
          1326,
          1327,
          1328,
          1329,
          1330,
          1331,
          1332,
          1333,
          1334,
          1335,
          1336,
          1337,
          1338,
          1339,
          1340,
          1341,
          1342,
          1343,
          1344,
          1345,
          1346,
          1347,
          1348,
          1349,
          1350,
          1351,
          1352,
          1353,
          1354,
          1355,
          1356,
          1357,
          1358,
          1359,
          1360,
          1361,
          1362,
          1363,
          1364,
          1365,
          1366,
          1367,
          1368,
          1369,
          1370,
          1371,
          1372,
          1373,
          1374,
          1375,
          1376,
          1377,
          1378,
          1379,
          1380,
          1381,
          1382,
          1383,
          1384,
          1385,
          1386,
          1387,
          1388,
          1389,
          1390,
          1391,
          1392,
          1393,
          1394,
          1395,
          1396,
          1397,
          1398,
          1399,
          1400,
          1401,
          1402,
          1403,
          1404,
          1405,
          1406,
          1407,
          1408,
          1409,
          1410,
          1411,
          1412,
          1413,
          1414,
          1415,
          1416,
          1417,
          1418,
          1419,
          1420,
          1421,
          1422,
          1423,
          1424,
          1425,
          1426,
          1427,
          1428,
          1429,
          1430,
          1431,
          1432,
          1433,
          1434,
          1435,
          1436,
          1437,
          1438,
          1439,
          1440,
          1441,
          1442,
          1443,
          1444,
          1445,
          1446,
          1447,
          1448,
          1449,
          1450,
          1451,
          1452,
          1453,
          1454,
          1455,
          1456,
          1457,
          1458,
          1459,
          1460,
          1461,
          1462,
          1463,
          1464,
          1465,
          1466,
          1467,
          1468,
          1469,
          1470,
          1471,
          1472,
          1473,
          1474,
          1475,
          1476,
          1477,
          1478,
          1479,
          1480,
          1481,
          1482,
          1483,
          1484,
          1485,
          1486,
          1487,
          1488,
          1489,
          1490,
          1491,
          1492,
          1493,
          1494,
          1495,
          1496,
          1497,
          1498,
          1499,
          1500,
          1501,
          1502,
          1503,
          1504,
          1505,
          1506,
          1507,
          1508,
          1509,
          1510,
          1511,
          1512,
          1513,
          1514,
          1515,
          1516,
          1517,
          1518,
          1519,
          1520,
          1521,
          1522,
          1523,
          1524,
          1525,
          1526,
          1527,
          1528,
          1529,
          1530,
          1531,
          1532,
          1533,
          1534,
          1535,
          1536,
          1537,
          1538,
          1539,
          1540,
          1541,
          1542,
          1543,
          1544,
          1545,
          1546,
          1547,
          1548,
          1549,
          1550,
          1551,
          1552,
          1553,
          1554,
          1555,
          1556,
          1557,
          1558,
          1559,
          1560,
          1561,
          1562,
          1563,
          1564,
          1565,
          1566,
          1567,
          1568,
          1569,
          1570,
          1571,
          1572,
          1573,
          1574,
          1575,
          1576,
          1577,
          1578,
          1579,
          1580,
          1581,
          1582,
          1583,
          1584,
          1585,
          1586,
          1587,
          1588,
          1589,
          1590,
          1591,
          1592,
          1593,
          1594,
          1595,
          1596,
          1597,
          1598,
          1599,
          1600,
          1601,
          1602,
          1603,
          1604,
          1605,
          1606,
          1607,
          1608,
          1609,
          1610,
          1611,
          1612,
          1613,
          1614,
          1615,
          1616,
          1617,
          1618,
          1619,
          1620,
          1621,
          1622,
          1623,
          1624,
          1625,
          1626,
          1627,
          1628,
          1629,
          1630,
          1631,
          1632,
          1633,
          1634,
          1635,
          1636,
          1637,
          1638,
          1639,
          1640,
          1641,
          1642,
          1643,
          1644,
          1645,
          1646,
          1647,
          1648,
          1649,
          1650,
          1651,
          1652,
          1653,
          1654,
          1655,
          1656,
          1657,
          1658,
          1659,
          1660,
          1661,
          1662,
          1663,
          1664,
          1665,
          1666,
          1667,
          1668,
          1669,
          1670,
          1671,
          1672,
          1673,
          1674,
          1675,
          1676,
          1677,
          1678,
          1679,
          1680,
          1681,
          1682,
          1683,
          1684,
          1685,
          1686,
          1687,
          1688,
          1689,
          1690,
          1691,
          1692,
          1693,
          1694,
          1695,
          1696,
          1697,
          1698,
          1699,
          1700,
          1701,
          1702,
          1703,
          1704,
          1705,
          1706,
          1707,
          1708,
          1709,
          1710,
          1711,
          1712,
          1713,
          1714,
          1715,
          1716,
          1717,
          1718,
          1719,
          1720,
          1721,
          1722,
          1723,
          1724,
          1725,
          1726,
          1727,
          1728,
          1729,
          1730,
          1731,
          1732,
          1733,
          1734,
          1735,
          1736,
          1737,
          1738,
          1739,
          1740,
          1741,
          1742,
          1743,
          1744,
          1745,
          1746,
          1747,
          1748,
          1749,
          1750,
          1751,
          1752,
          1753,
          1754,
          1755,
          1756,
          1757,
          1758,
          1759,
          1760,
          1761,
          1762,
          1763,
          1764,
          1765,
          1766,
          1767,
          1768,
          1769,
          1770,
          1771,
          1772,
          1773,
          1774,
          1775,
          1776,
          1777,
          1778,
          1779,
          1780,
          1781,
          1782,
          1783,
          1784,
          1785,
          1786,
          1787,
          1788,
          1789,
          1790,
          1791,
          1792,
          1793,
          1794,
          1795,
          1796,
          1797,
          1798,
          1799,
          1800,
          1801,
          1802,
          1803,
          1804,
          1805,
          1806,
          1807,
          1808,
          1809,
          1810,
          1811,
          1812,
          1813,
          1814,
          1815,
          1816,
          1817,
          1818,
          1819,
          1820,
          1821,
          1822,
          1823,
          1824,
          1825,
          1826,
          1827,
          1828,
          1829,
          1830,
          1831,
          1832,
          1833,
          1834,
          1835,
          1836,
          1837,
          1838,
          1839,
          1840,
          1841,
          1842,
          1843,
          1844,
          1845,
          1846,
          1847,
          1848,
          1849,
          1850,
          1851,
          1852,
          1853,
          1854,
          1855,
          1856,
          1857,
          1858,
          1859,
          1860,
          1861,
          1862,
          1863,
          1864,
          1865,
          1866,
          1867,
          1868,
          1869,
          1870,
          1871,
          1872,
          1873,
          1874,
          1875,
          1876,
          1877,
          1878,
          1879,
          1880,
          1881,
          1882,
          1883,
          1884,
          1885,
          1886,
          1887,
          1888,
          1889,
          1890,
          1891,
          1892,
          1893,
          1894,
          1895,
          1896,
          1897,
          1898,
          1899,
          1900,
          1901,
          1902,
          1903,
          1904,
          1905,
          1906,
          1907,
          1908,
          1909,
          1910,
          1911,
          1912,
          1913,
          1914,
          1915,
          1916,
          1917,
          1918,
          1919,
          1920,
          1921,
          1922,
          1923,
          1924,
          1925,
          1926,
          1927,
          1928,
          1929,
          1930,
          1931,
          1932,
          1933,
          1934,
          1935,
          1936,
          1937,
          1938,
          1939,
          1940,
          1941,
          1942,
          1943,
          1944,
          1945,
          1946,
          1947,
          1948,
          1949,
          1950,
          1951,
          1952,
          1953,
          1954,
          1955,
          1956,
          1957,
          1958,
          1959,
          1960,
          1961,
          1962,
          1963,
          1964,
          1965,
          1966,
          1967,
          1968,
          1969,
          1970,
          1971,
          1972,
          1973,
          1974,
          1975,
          1976,
          1977,
          1978,
          1979,
          1980,
          1981,
          1982,
          1983,
          1984,
          1985,
          1986,
          1987,
          1988,
          1989,
          1990,
          1991,
          1992,
          1993,
          1994,
          1995,
          1996,
          1997,
          1998,
          1999,
          2000,
          2001,
          2002,
          2003,
          2004,
          2005,
          2006,
          2007,
          2008,
          2009,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016,
          2017,
          2018,
          2019,
          2020,
          2021,
          2022,
          2023,
          2024,
          2025,
          2026,
          2027,
          2028,
          2029,
          2030,
          2031,
          2032,
          2033,
          2034,
          2035,
          2036,
          2037,
          2038,
          2039,
          2040,
          2041,
          2042,
          2043,
          2044,
          2045,
          2046,
          2047,
          2048,
          2049,
          2050,
          2051,
          2052,
          2053,
          2054,
          2055,
          2056,
          2057,
          2058,
          2059,
          2060,
          2061,
          2062,
          2063,
          2064,
          2065,
          2066,
          2067,
          2068,
          2069,
          2070,
          2071,
          2072,
          2073,
          2074,
          2075,
          2076,
          2077,
          2078,
          2079,
          2080,
          2081,
          2082,
          2083,
          2084,
          2085,
          2086,
          2087,
          2088,
          2089,
          2090,
          2091,
          2092,
          2093,
          2094,
          2095,
          2096,
          2097,
          2098,
          2099,
          2100,
          2101,
          2102,
          2103,
          2104,
          2105,
          2106,
          2107,
          2108,
          2109,
          2110,
          2111,
          2112,
          2113,
          2114,
          2115,
          2116,
          2117,
          2118,
          2119,
          2120,
          2121,
          2122,
          2123,
          2124,
          2125,
          2126,
          2127,
          2128,
          2129,
          2130,
          2131,
          2132,
          2133,
          2134,
          2135,
          2136,
          2137,
          2138,
          2139,
          2140,
          2141,
          2142,
          2143,
          2144,
          2145,
          2146,
          2147,
          2148,
          2149,
          2150,
          2151,
          2152,
          2153,
          2154,
          2155,
          2156,
          2157,
          2158,
          2159,
          2160,
          2161,
          2162,
          2163,
          2164,
          2165,
          2166,
          2167,
          2168,
          2169,
          2170,
          2171,
          2172,
          2173,
          2174,
          2175,
          2176,
          2177,
          2178,
          2179,
          2180,
          2181,
          2182,
          2183,
          2184,
          2185,
          2186,
          2187,
          2188,
          2189,
          2190,
          2191,
          2192,
          2193,
          2194,
          2195,
          2196,
          2197,
          2198,
          2199,
          2200,
          2201,
          2202,
          2203,
          2204,
          2205,
          2206,
          2207,
          2208,
          2209,
          2210,
          2211,
          2212,
          2213,
          2214,
          2215,
          2216,
          2217,
          2218,
          2219,
          2220,
          2221,
          2222,
          2223,
          2224,
          2225,
          2226,
          2227,
          2228,
          2229,
          2230,
          2231,
          2232,
          2233,
          2234,
          2235,
          2236,
          2237,
          2238,
          2239,
          2240,
          2241,
          2242,
          2243,
          2244,
          2245,
          2246,
          2247,
          2248,
          2249,
          2250,
          2251,
          2252,
          2253,
          2254,
          2255,
          2256,
          2257,
          2258,
          2259,
          2260,
          2261,
          2262,
          2263,
          2264,
          2265,
          2266,
          2267,
          2268,
          2269,
          2270,
          2271,
          2272,
          2273,
          2274,
          2275,
          2276,
          2277,
          2278,
          2279,
          2280,
          2281,
          2282,
          2283,
          2284,
          2285,
          2286,
          2287,
          2288,
          2289,
          2290,
          2291,
          2292,
          2293,
          2294,
          2295,
          2296,
          2297,
          2298,
          2299,
          2300,
          2301,
          2302,
          2303,
          2304,
          2305,
          2306,
          2307,
          2308,
          2309,
          2310,
          2311,
          2312,
          2313,
          2314,
          2315,
          2316,
          2317,
          2318,
          2319,
          2320,
          2321,
          2322,
          2323,
          2324,
          2325,
          2326,
          2327,
          2328,
          2329,
          2330,
          2331,
          2332,
          2333,
          2334,
          2335,
          2336,
          2337,
          2338,
          2339,
          2340,
          2341,
          2342,
          2343,
          2344,
          2345,
          2346,
          2347,
          2348,
          2349,
          2350,
          2351,
          2352,
          2353,
          2354,
          2355,
          2356,
          2357,
          2358,
          2359,
          2360,
          2361,
          2362,
          2363,
          2364,
          2365,
          2366,
          2367,
          2368,
          2369,
          2370,
          2371,
          2372,
          2373,
          2374,
          2375,
          2376,
          2377,
          2378,
          2379,
          2380,
          2381,
          2382,
          2383,
          2384,
          2385,
          2386,
          2387,
          2388,
          2389,
          2390,
          2391,
          2392,
          2393,
          2394,
          2395,
          2396,
          2397,
          2398,
          2399,
          2400,
          2401,
          2402,
          2403,
          2404,
          2405,
          2406,
          2407,
          2408,
          2409,
          2410,
          2411,
          2412,
          2413,
          2414,
          2415,
          2416,
          2417,
          2418,
          2419,
          2420,
          2421,
          2422,
          2423,
          2424,
          2425,
          2426,
          2427,
          2428,
          2429,
          2430,
          2431,
          2432,
          2433,
          2434,
          2435,
          2436,
          2437,
          2438,
          2439,
          2440,
          2441,
          2442,
          2443,
          2444,
          2445,
          2446,
          2447,
          2448,
          2449,
          2450,
          2451,
          2452,
          2453,
          2454,
          2455,
          2456,
          2457,
          2458,
          2459,
          2460,
          2461,
          2462,
          2463,
          2464,
          2465,
          2466,
          2467,
          2468,
          2469,
          2470,
          2471,
          2472,
          2473,
          2474,
          2475,
          2476,
          2477,
          2478,
          2479,
          2480,
          2481,
          2482,
          2483,
          2484,
          2485,
          2486,
          2487,
          2488,
          2489,
          2490,
          2491,
          2492,
          2493,
          2494,
          2495,
          2496,
          2497,
          2498,
          2499,
          2500,
          2501,
          2502,
          2503,
          2504,
          2505,
          2506,
          2507,
          2508,
          2509,
          2510,
          2511,
          2512,
          2513,
          2514,
          2515,
          2516,
          2517,
          2518,
          2519,
          2520,
          2521,
          2522,
          2523,
          2524,
          2525,
          2526,
          2527,
          2528,
          2529,
          2530,
          2531,
          2532,
          2533,
          2534,
          2535,
          2536,
          2537,
          2538,
          2539,
          2540,
          2541,
          2542,
          2543,
          2544,
          2545,
          2546,
          2547,
          2548,
          2549,
          2550,
          2551,
          2552,
          2553,
          2554,
          2555,
          2556,
          2557,
          2558,
          2559,
          2560,
          2561,
          2562,
          2563,
          2564,
          2565,
          2566,
          2567,
          2568,
          2569,
          2570,
          2571,
          2572,
          2573,
          2574,
          2575,
          2576,
          2577,
          2578,
          2579,
          2580,
          2581,
          2582,
          2583,
          2584,
          2585,
          2586,
          2587,
          2588,
          2589,
          2590,
          2591,
          2592,
          2593,
          2594,
          2595,
          2596,
          2597,
          2598,
          2599,
          2600,
          2601,
          2602,
          2603,
          2604,
          2605,
          2606,
          2607,
          2608,
          2609,
          2610,
          2611,
          2612,
          2613,
          2614,
          2615,
          2616,
          2617,
          2618,
          2619,
          2620,
          2621,
          2622,
          2623,
          2624,
          2625,
          2626,
          2627,
          2628,
          2629,
          2630,
          2631,
          2632,
          2633,
          2634,
          2635,
          2636,
          2637,
          2638,
          2639,
          2640,
          2641,
          2642,
          2643,
          2644,
          2645,
          2646,
          2647,
          2648,
          2649,
          2650,
          2651,
          2652,
          2653,
          2654,
          2655,
          2656,
          2657,
          2658,
          2659,
          2660,
          2661,
          2662,
          2663,
          2664,
          2665,
          2666,
          2667,
          2668,
          2669,
          2670,
          2671,
          2672,
          2673,
          2674,
          2675,
          2676,
          2677,
          2678,
          2679,
          2680,
          2681,
          2682,
          2683,
          2684,
          2685,
          2686,
          2687,
          2688,
          2689,
          2690,
          2691,
          2692,
          2693,
          2694,
          2695,
          2696,
          2697,
          2698,
          2699,
          2700,
          2701,
          2702,
          2703,
          2704,
          2705,
          2706,
          2707,
          2708,
          2709,
          2710,
          2711,
          2712,
          2713,
          2714,
          2715,
          2716,
          2717,
          2718,
          2719,
          2720,
          2721,
          2722,
          2723,
          2724,
          2725,
          2726,
          2727,
          2728,
          2729,
          2730,
          2731,
          2732,
          2733,
          2734,
          2735,
          2736,
          2737,
          2738,
          2739,
          2740,
          2741,
          2742,
          2743,
          2744,
          2745,
          2746,
          2747,
          2748,
          2749,
          2750,
          2751,
          2752,
          2753,
          2754,
          2755,
          2756,
          2757,
          2758,
          2759,
          2760,
          2761,
          2762,
          2763,
          2764,
          2765,
          2766,
          2767,
          2768,
          2769,
          2770,
          2771,
          2772,
          2773,
          2774,
          2775,
          2776,
          2777,
          2778,
          2779,
          2780,
          2781,
          2782,
          2783,
          2784,
          2785,
          2786,
          2787,
          2788,
          2789,
          2790,
          2791,
          2792,
          2793,
          2794,
          2795,
          2796,
          2797,
          2798,
          2799,
          2800,
          2801,
          2802,
          2803,
          2804,
          2805,
          2806,
          2807,
          2808,
          2809,
          2810,
          2811,
          2812,
          2813,
          2814,
          2815,
          2816,
          2817,
          2818,
          2819,
          2820,
          2821,
          2822,
          2823,
          2824,
          2825,
          2826,
          2827,
          2828,
          2829,
          2830,
          2831,
          2832,
          2833,
          2834,
          2835,
          2836,
          2837,
          2838,
          2839,
          2840,
          2841,
          2842,
          2843,
          2844,
          2845,
          2846,
          2847,
          2848,
          2849,
          2850,
          2851,
          2852,
          2853,
          2854,
          2855,
          2856,
          2857,
          2858,
          2859,
          2860,
          2861,
          2862,
          2863,
          2864,
          2865,
          2866,
          2867,
          2868,
          2869,
          2870,
          2871,
          2872,
          2873,
          2874,
          2875,
          2876,
          2877,
          2878,
          2879,
          2880,
          2881,
          2882,
          2883,
          2884,
          2885,
          2886,
          2887,
          2888,
          2889,
          2890,
          2891,
          2892,
          2893,
          2894,
          2895,
          2896,
          2897,
          2898,
          2899,
          2900,
          2901,
          2902,
          2903,
          2904,
          2905,
          2906,
          2907,
          2908,
          2909,
          2910,
          2911,
          2912,
          2913,
          2914,
          2915,
          2916,
          2917,
          2918,
          2919,
          2920,
          2921,
          2922,
          2923,
          2924,
          2925,
          2926,
          2927,
          2928,
          2929,
          2930,
          2931,
          2932,
          2933,
          2934,
          2935,
          2936,
          2937,
          2938,
          2939,
          2940,
          2941,
          2942,
          2943,
          2944,
          2945,
          2946,
          2947,
          2948,
          2949,
          2950,
          2951,
          2952,
          2953,
          2954,
          2955,
          2956,
          2957,
          2958,
          2959,
          2960,
          2961,
          2962,
          2963,
          2964,
          2965,
          2966,
          2967,
          2968,
          2969,
          2970,
          2971,
          2972,
          2973,
          2974,
          2975,
          2976,
          2977,
          2978,
          2979,
          2980,
          2981,
          2982,
          2983,
          2984,
          2985,
          2986,
          2987,
          2988,
          2989,
          2990,
          2991,
          2992,
          2993,
          2994,
          2995,
          2996,
          2997,
          2998,
          2999,
          3000,
          3001,
          3002,
          3003,
          3004,
          3005,
          3006,
          3007,
          3008,
          3009,
          3010,
          3011,
          3012,
          3013,
          3014,
          3015,
          3016,
          3017,
          3018,
          3019,
          3020,
          3021,
          3022,
          3023,
          3024,
          3025,
          3026,
          3027,
          3028,
          3029,
          3030,
          3031,
          3032,
          3033,
          3034,
          3035,
          3036,
          3037,
          3038,
          3039,
          3040,
          3041,
          3042,
          3043,
          3044,
          3045,
          3046,
          3047,
          3048,
          3049,
          3050,
          3051,
          3052,
          3053,
          3054,
          3055,
          3056,
          3057,
          3058,
          3059,
          3060,
          3061,
          3062,
          3063,
          3064,
          3065,
          3066,
          3067,
          3068,
          3069,
          3070,
          3071,
          3072,
          3073,
          3074,
          3075,
          3076,
          3077,
          3078,
          3079,
          3080,
          3081,
          3082,
          3083,
          3084,
          3085,
          3086,
          3087,
          3088,
          3089,
          3090,
          3091,
          3092,
          3093,
          3094,
          3095,
          3096,
          3097,
          3098,
          3099,
          3100,
          3101,
          3102,
          3103,
          3104,
          3105,
          3106,
          3107,
          3108,
          3109,
          3110,
          3111,
          3112,
          3113,
          3114,
          3115,
          3116,
          3117,
          3118,
          3119,
          3120,
          3121,
          3122,
          3123,
          3124,
          3125,
          3126,
          3127,
          3128,
          3129,
          3130,
          3131,
          3132,
          3133,
          3134,
          3135,
          3136,
          3137,
          3138,
          3139,
          3140,
          3141,
          3142,
          3143,
          3144,
          3145,
          3146,
          3147,
          3148,
          3149,
          3150,
          3151,
          3152,
          3153,
          3154,
          3155,
          3156,
          3157,
          3158,
          3159,
          3160,
          3161,
          3162,
          3163,
          3164,
          3165,
          3166,
          3167,
          3168,
          3169,
          3170,
          3171,
          3172,
          3173,
          3174,
          3175,
          3176,
          3177,
          3178,
          3179,
          3180,
          3181,
          3182,
          3183,
          3184,
          3185,
          3186,
          3187,
          3188,
          3189,
          3190,
          3191,
          3192,
          3193,
          3194,
          3195,
          3196,
          3197,
          3198,
          3199,
          3200,
          3201,
          3202,
          3203,
          3204,
          3205,
          3206,
          3207,
          3208,
          3209,
          3210,
          3211,
          3212,
          3213,
          3214,
          3215,
          3216,
          3217,
          3218,
          3219,
          3220,
          3221,
          3222,
          3223,
          3224,
          3225,
          3226,
          3227,
          3228,
          3229,
          3230,
          3231,
          3232,
          3233,
          3234,
          3235,
          3236,
          3237,
          3238,
          3239,
          3240,
          3241,
          3242,
          3243,
          3244,
          3245,
          3246,
          3247,
          3248,
          3249,
          3250,
          3251,
          3252,
          3253,
          3254,
          3255,
          3256,
          3257,
          3258,
          3259,
          3260,
          3261,
          3262,
          3263,
          3264,
          3265,
          3266,
          3267,
          3268,
          3269,
          3270,
          3271,
          3272,
          3273,
          3274,
          3275,
          3276,
          3277,
          3278,
          3279,
          3280,
          3281,
          3282,
          3283,
          3284,
          3285,
          3286,
          3287,
          3288,
          3289,
          3290,
          3291,
          3292,
          3293,
          3294,
          3295,
          3296,
          3297,
          3298,
          3299,
          3300,
          3301,
          3302,
          3303,
          3304,
          3305,
          3306,
          3307,
          3308,
          3309,
          3310,
          3311,
          3312,
          3313,
          3314,
          3315,
          3316,
          3317,
          3318,
          3319,
          3320,
          3321,
          3322,
          3323,
          3324,
          3325,
          3326,
          3327,
          3328,
          3329,
          3330,
          3331,
          3332,
          3333,
          3334,
          3335,
          3336,
          3337,
          3338,
          3339,
          3340,
          3341,
          3342,
          3343,
          3344,
          3345,
          3346,
          3347,
          3348,
          3349,
          3350,
          3351,
          3352,
          3353,
          3354,
          3355,
          3356,
          3357,
          3358,
          3359,
          3360,
          3361,
          3362,
          3363,
          3364,
          3365,
          3366,
          3367,
          3368,
          3369,
          3370,
          3371,
          3372,
          3373,
          3374,
          3375,
          3376,
          3377,
          3378,
          3379,
          3380,
          3381,
          3382,
          3383,
          3384,
          3385,
          3386,
          3387,
          3388,
          3389,
          3390,
          3391,
          3392,
          3393,
          3394,
          3395,
          3396,
          3397,
          3398,
          3399,
          3400,
          3401,
          3402,
          3403,
          3404,
          3405,
          3406,
          3407,
          3408,
          3409,
          3410,
          3411,
          3412,
          3413,
          3414,
          3415,
          3416,
          3417,
          3418,
          3419,
          3420,
          3421,
          3422,
          3423,
          3424,
          3425,
          3426,
          3427,
          3428,
          3429,
          3430,
          3431,
          3432,
          3433,
          3434,
          3435,
          3436,
          3437,
          3438,
          3439,
          3440,
          3441,
          3442,
          3443,
          3444,
          3445,
          3446,
          3447,
          3448,
          3449,
          3450,
          3451,
          3452,
          3453,
          3454,
          3455,
          3456,
          3457,
          3458,
          3459,
          3460,
          3461,
          3462,
          3463,
          3464,
          3465,
          3466,
          3467,
          3468,
          3469,
          3470,
          3471,
          3472,
          3473,
          3474,
          3475,
          3476,
          3477,
          3478,
          3479,
          3480,
          3481,
          3482,
          3483,
          3484,
          3485,
          3486,
          3487,
          3488,
          3489,
          3490,
          3491,
          3492,
          3493,
          3494,
          3495,
          3496,
          3497,
          3498,
          3499,
          3500,
          3501,
          3502,
          3503,
          3504,
          3505,
          3506,
          3507,
          3508,
          3509,
          3510,
          3511,
          3512,
          3513,
          3514,
          3515,
          3516,
          3517,
          3518,
          3519,
          3520,
          3521,
          3522,
          3523,
          3524,
          3525,
          3526,
          3527,
          3528,
          3529,
          3530,
          3531,
          3532,
          3533,
          3534,
          3535,
          3536,
          3537,
          3538,
          3539,
          3540,
          3541,
          3542,
          3543,
          3544,
          3545,
          3546,
          3547,
          3548,
          3549,
          3550,
          3551,
          3552,
          3553,
          3554,
          3555,
          3556,
          3557,
          3558,
          3559,
          3560,
          3561,
          3562,
          3563,
          3564,
          3565,
          3566,
          3567,
          3568,
          3569,
          3570,
          3571,
          3572,
          3573,
          3574,
          3575,
          3576,
          3577,
          3578,
          3579,
          3580,
          3581,
          3582,
          3583,
          3584,
          3585,
          3586,
          3587,
          3588,
          3589,
          3590,
          3591,
          3592,
          3593,
          3594,
          3595,
          3596,
          3597,
          3598,
          3599,
          3600,
          3601,
          3602,
          3603,
          3604,
          3605,
          3606,
          3607,
          3608,
          3609,
          3610,
          3611,
          3612,
          3613,
          3614,
          3615,
          3616,
          3617,
          3618,
          3619,
          3620,
          3621,
          3622,
          3623,
          3624,
          3625,
          3626,
          3627,
          3628,
          3629,
          3630,
          3631,
          3632,
          3633,
          3634,
          3635,
          3636,
          3637,
          3638,
          3639,
          3640,
          3641,
          3642,
          3643,
          3644,
          3645,
          3646,
          3647,
          3648,
          3649,
          3650,
          3651,
          3652,
          3653,
          3654,
          3655,
          3656,
          3657,
          3658,
          3659,
          3660,
          3661,
          3662,
          3663,
          3664,
          3665,
          3666,
          3667,
          3668,
          3669,
          3670,
          3671,
          3672,
          3673,
          3674,
          3675,
          3676,
          3677,
          3678,
          3679,
          3680,
          3681,
          3682,
          3683,
          3684,
          3685,
          3686,
          3687,
          3688,
          3689,
          3690,
          3691,
          3692,
          3693,
          3694,
          3695,
          3696,
          3697,
          3698,
          3699,
          3700,
          3701,
          3702,
          3703,
          3704,
          3705,
          3706,
          3707,
          3708,
          3709,
          3710,
          3711,
          3712,
          3713,
          3714,
          3715,
          3716,
          3717,
          3718,
          3719,
          3720,
          3721,
          3722,
          3723,
          3724,
          3725,
          3726,
          3727,
          3728,
          3729,
          3730,
          3731,
          3732,
          3733,
          3734,
          3735,
          3736,
          3737,
          3738,
          3739,
          3740,
          3741,
          3742,
          3743,
          3744,
          3745,
          3746,
          3747,
          3748,
          3749
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "coloraxis": {
         "colorbar": {
          "title": {
           "text": "color"
          }
         },
         "colorscale": [
          [
           0,
           "#0d0887"
          ],
          [
           0.1111111111111111,
           "#46039f"
          ],
          [
           0.2222222222222222,
           "#7201a8"
          ],
          [
           0.3333333333333333,
           "#9c179e"
          ],
          [
           0.4444444444444444,
           "#bd3786"
          ],
          [
           0.5555555555555556,
           "#d8576b"
          ],
          [
           0.6666666666666666,
           "#ed7953"
          ],
          [
           0.7777777777777778,
           "#fb9f3a"
          ],
          [
           0.8888888888888888,
           "#fdca26"
          ],
          [
           1,
           "#f0f921"
          ]
         ]
        },
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(X_kpca[:,0], X_kpca[:,1])\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(X_kpca[:,0], X_kpca[:,1], color=y[0])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9490, 510)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal distribution test\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "stat, p = normaltest(X)\n",
    "stat, p\n",
    "\n",
    "normal = 0\n",
    "not_normal = 0\n",
    "\n",
    "# hipothesis : x comes from a normal distribution\n",
    "\n",
    "for i in p:\n",
    "    if i < 0.05:\n",
    "        not_normal += 1 # hypothesis can be rejected\n",
    "    else:\n",
    "        normal += 1 # hypothesis can not be rejected\n",
    "\n",
    "normal, not_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82425654, 0.65432097, 0.98708188, ..., 0.08635164, 0.4635971 ,\n",
       "       0.80054311])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarization\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scal = scaler.fit_transform(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rem = ho.removing_iqr(pd.DataFrame(X_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = X_rem.sum().sum() / (X_rem.shape[0] * X_rem.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007063386666666667"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = ho.mask_outliers(X_copy, X_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced = ho.replace_missing_values(masked, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\n",
    "ax1.set_title('Distribution after outliers')\n",
    "\n",
    "for i in replaced:\n",
    "    sns.kdeplot(replaced[i], ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binar = binarize(y)\n",
    "y = pd.DataFrame(np.ravel(y_binar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA that will retain 95% of the variance\n",
    "pca = PCA(n_components=0.95, whiten=True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_pca = pca.fit_transform(replaced)\n",
    "X_pca = pd.DataFrame(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9990</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304.3610</td>\n",
       "      <td>-436.590</td>\n",
       "      <td>-33854.769</td>\n",
       "      <td>-96571.569</td>\n",
       "      <td>-15086.947</td>\n",
       "      <td>-794.409</td>\n",
       "      <td>-442.859</td>\n",
       "      <td>304.248</td>\n",
       "      <td>-202.411</td>\n",
       "      <td>-506.852</td>\n",
       "      <td>...</td>\n",
       "      <td>95.294</td>\n",
       "      <td>-3.936</td>\n",
       "      <td>-251.812</td>\n",
       "      <td>256.429</td>\n",
       "      <td>-473.661</td>\n",
       "      <td>-1398.604</td>\n",
       "      <td>42.638</td>\n",
       "      <td>771.185</td>\n",
       "      <td>-191.123</td>\n",
       "      <td>1356.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0470</td>\n",
       "      <td>90.087</td>\n",
       "      <td>-154.747</td>\n",
       "      <td>-4116.486</td>\n",
       "      <td>38365.133</td>\n",
       "      <td>-589.309</td>\n",
       "      <td>1115.367</td>\n",
       "      <td>274.199</td>\n",
       "      <td>814.953</td>\n",
       "      <td>167.272</td>\n",
       "      <td>...</td>\n",
       "      <td>-176.341</td>\n",
       "      <td>-426.238</td>\n",
       "      <td>-542.428</td>\n",
       "      <td>99.147</td>\n",
       "      <td>-102.309</td>\n",
       "      <td>-277.948</td>\n",
       "      <td>-98.859</td>\n",
       "      <td>-1163.779</td>\n",
       "      <td>265.231</td>\n",
       "      <td>-992.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272.1680</td>\n",
       "      <td>-201.736</td>\n",
       "      <td>4212.592</td>\n",
       "      <td>-9123.655</td>\n",
       "      <td>-35422.458</td>\n",
       "      <td>-1029.249</td>\n",
       "      <td>-506.290</td>\n",
       "      <td>-476.856</td>\n",
       "      <td>-570.731</td>\n",
       "      <td>-161.288</td>\n",
       "      <td>...</td>\n",
       "      <td>321.426</td>\n",
       "      <td>-376.033</td>\n",
       "      <td>142.834</td>\n",
       "      <td>920.958</td>\n",
       "      <td>-122.525</td>\n",
       "      <td>-174.304</td>\n",
       "      <td>-137.612</td>\n",
       "      <td>-1571.473</td>\n",
       "      <td>678.323</td>\n",
       "      <td>1020.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170.0070</td>\n",
       "      <td>68.736</td>\n",
       "      <td>48662.079</td>\n",
       "      <td>29735.235</td>\n",
       "      <td>-13903.955</td>\n",
       "      <td>895.081</td>\n",
       "      <td>-257.748</td>\n",
       "      <td>-811.058</td>\n",
       "      <td>-691.561</td>\n",
       "      <td>-31.439</td>\n",
       "      <td>...</td>\n",
       "      <td>-409.919</td>\n",
       "      <td>400.946</td>\n",
       "      <td>313.270</td>\n",
       "      <td>123.172</td>\n",
       "      <td>1786.962</td>\n",
       "      <td>147.637</td>\n",
       "      <td>31.433</td>\n",
       "      <td>-784.719</td>\n",
       "      <td>573.433</td>\n",
       "      <td>1454.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.7970</td>\n",
       "      <td>180.052</td>\n",
       "      <td>-49643.545</td>\n",
       "      <td>2515.406</td>\n",
       "      <td>26206.315</td>\n",
       "      <td>-407.453</td>\n",
       "      <td>-189.416</td>\n",
       "      <td>-53.664</td>\n",
       "      <td>-159.507</td>\n",
       "      <td>-42.291</td>\n",
       "      <td>...</td>\n",
       "      <td>-101.761</td>\n",
       "      <td>-424.898</td>\n",
       "      <td>37.254</td>\n",
       "      <td>-337.431</td>\n",
       "      <td>423.691</td>\n",
       "      <td>14.240</td>\n",
       "      <td>267.352</td>\n",
       "      <td>-234.560</td>\n",
       "      <td>-213.804</td>\n",
       "      <td>873.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>338.5390</td>\n",
       "      <td>-98.216</td>\n",
       "      <td>-37488.883</td>\n",
       "      <td>-2346.403</td>\n",
       "      <td>-291.325</td>\n",
       "      <td>553.155</td>\n",
       "      <td>1041.511</td>\n",
       "      <td>391.664</td>\n",
       "      <td>1016.730</td>\n",
       "      <td>49.772</td>\n",
       "      <td>...</td>\n",
       "      <td>350.501</td>\n",
       "      <td>-607.873</td>\n",
       "      <td>430.407</td>\n",
       "      <td>-469.737</td>\n",
       "      <td>68.670</td>\n",
       "      <td>328.636</td>\n",
       "      <td>105.448</td>\n",
       "      <td>-368.834</td>\n",
       "      <td>378.017</td>\n",
       "      <td>-253.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>105.5110</td>\n",
       "      <td>-167.468</td>\n",
       "      <td>54447.468</td>\n",
       "      <td>-38055.924</td>\n",
       "      <td>-12394.035</td>\n",
       "      <td>711.218</td>\n",
       "      <td>100.459</td>\n",
       "      <td>-1377.005</td>\n",
       "      <td>-171.175</td>\n",
       "      <td>-325.444</td>\n",
       "      <td>...</td>\n",
       "      <td>4.759</td>\n",
       "      <td>-9.079</td>\n",
       "      <td>104.333</td>\n",
       "      <td>4.676</td>\n",
       "      <td>-1069.879</td>\n",
       "      <td>-92.252</td>\n",
       "      <td>86.110</td>\n",
       "      <td>-4.935</td>\n",
       "      <td>433.674</td>\n",
       "      <td>-355.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>93.1860</td>\n",
       "      <td>-155.288</td>\n",
       "      <td>-16555.534</td>\n",
       "      <td>-10605.421</td>\n",
       "      <td>-3027.792</td>\n",
       "      <td>-80.498</td>\n",
       "      <td>-2201.598</td>\n",
       "      <td>-49.392</td>\n",
       "      <td>-848.946</td>\n",
       "      <td>-252.714</td>\n",
       "      <td>...</td>\n",
       "      <td>-342.855</td>\n",
       "      <td>-151.956</td>\n",
       "      <td>7.386</td>\n",
       "      <td>564.910</td>\n",
       "      <td>613.541</td>\n",
       "      <td>-570.735</td>\n",
       "      <td>-5.810</td>\n",
       "      <td>797.659</td>\n",
       "      <td>-289.170</td>\n",
       "      <td>-91.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>103.6948</td>\n",
       "      <td>98.182</td>\n",
       "      <td>-39429.721</td>\n",
       "      <td>26215.357</td>\n",
       "      <td>-1465.352</td>\n",
       "      <td>-340.496</td>\n",
       "      <td>-992.109</td>\n",
       "      <td>990.646</td>\n",
       "      <td>140.943</td>\n",
       "      <td>-281.100</td>\n",
       "      <td>...</td>\n",
       "      <td>200.827</td>\n",
       "      <td>-1445.572</td>\n",
       "      <td>-39.879</td>\n",
       "      <td>421.075</td>\n",
       "      <td>-191.389</td>\n",
       "      <td>-538.616</td>\n",
       "      <td>141.365</td>\n",
       "      <td>552.974</td>\n",
       "      <td>310.130</td>\n",
       "      <td>207.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>-169.3940</td>\n",
       "      <td>127.276</td>\n",
       "      <td>-24661.781</td>\n",
       "      <td>37020.744</td>\n",
       "      <td>-17491.827</td>\n",
       "      <td>46.510</td>\n",
       "      <td>560.459</td>\n",
       "      <td>155.117</td>\n",
       "      <td>-217.523</td>\n",
       "      <td>-448.290</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.249</td>\n",
       "      <td>-419.265</td>\n",
       "      <td>224.068</td>\n",
       "      <td>-75.648</td>\n",
       "      <td>-302.584</td>\n",
       "      <td>-164.119</td>\n",
       "      <td>19.465</td>\n",
       "      <td>-841.065</td>\n",
       "      <td>-973.910</td>\n",
       "      <td>-803.150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2          3          4         5         6     \\\n",
       "0     304.3610 -436.590 -33854.769 -96571.569 -15086.947  -794.409  -442.859   \n",
       "1      54.0470   90.087   -154.747  -4116.486  38365.133  -589.309  1115.367   \n",
       "2     272.1680 -201.736   4212.592  -9123.655 -35422.458 -1029.249  -506.290   \n",
       "3     170.0070   68.736  48662.079  29735.235 -13903.955   895.081  -257.748   \n",
       "4      86.7970  180.052 -49643.545   2515.406  26206.315  -407.453  -189.416   \n",
       "...        ...      ...        ...        ...        ...       ...       ...   \n",
       "3745  338.5390  -98.216 -37488.883  -2346.403   -291.325   553.155  1041.511   \n",
       "3746  105.5110 -167.468  54447.468 -38055.924 -12394.035   711.218   100.459   \n",
       "3747   93.1860 -155.288 -16555.534 -10605.421  -3027.792   -80.498 -2201.598   \n",
       "3748  103.6948   98.182 -39429.721  26215.357  -1465.352  -340.496  -992.109   \n",
       "3749 -169.3940  127.276 -24661.781  37020.744 -17491.827    46.510   560.459   \n",
       "\n",
       "          7         8        9     ...     9990      9991     9992     9993  \\\n",
       "0      304.248  -202.411 -506.852  ...   95.294    -3.936 -251.812  256.429   \n",
       "1      274.199   814.953  167.272  ... -176.341  -426.238 -542.428   99.147   \n",
       "2     -476.856  -570.731 -161.288  ...  321.426  -376.033  142.834  920.958   \n",
       "3     -811.058  -691.561  -31.439  ... -409.919   400.946  313.270  123.172   \n",
       "4      -53.664  -159.507  -42.291  ... -101.761  -424.898   37.254 -337.431   \n",
       "...        ...       ...      ...  ...      ...       ...      ...      ...   \n",
       "3745   391.664  1016.730   49.772  ...  350.501  -607.873  430.407 -469.737   \n",
       "3746 -1377.005  -171.175 -325.444  ...    4.759    -9.079  104.333    4.676   \n",
       "3747   -49.392  -848.946 -252.714  ... -342.855  -151.956    7.386  564.910   \n",
       "3748   990.646   140.943 -281.100  ...  200.827 -1445.572  -39.879  421.075   \n",
       "3749   155.117  -217.523 -448.290  ...  -87.249  -419.265  224.068  -75.648   \n",
       "\n",
       "          9994      9995     9996      9997     9998      9999  \n",
       "0     -473.661 -1398.604   42.638   771.185 -191.123  1356.137  \n",
       "1     -102.309  -277.948  -98.859 -1163.779  265.231  -992.056  \n",
       "2     -122.525  -174.304 -137.612 -1571.473  678.323  1020.263  \n",
       "3     1786.962   147.637   31.433  -784.719  573.433  1454.415  \n",
       "4      423.691    14.240  267.352  -234.560 -213.804   873.391  \n",
       "...        ...       ...      ...       ...      ...       ...  \n",
       "3745    68.670   328.636  105.448  -368.834  378.017  -253.044  \n",
       "3746 -1069.879   -92.252   86.110    -4.935  433.674  -355.118  \n",
       "3747   613.541  -570.735   -5.810   797.659 -289.170   -91.306  \n",
       "3748  -191.389  -538.616  141.365   552.974  310.130   207.084  \n",
       "3749  -302.584  -164.119   19.465  -841.065 -973.910  -803.150  \n",
       "\n",
       "[3750 rows x 10000 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 9974)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "stat, p = normaltest(replaced)\n",
    "stat, p\n",
    "\n",
    "normal = 0\n",
    "not_normal = 0\n",
    "\n",
    "# hypothesis : x comes from a normal distribution\n",
    "\n",
    "for i in p:\n",
    "    if i < 0.05:\n",
    "        not_normal += 1 # hypothesis can be rejected\n",
    "    else:\n",
    "        normal += 1 # hypothesis can not be rejected\n",
    "\n",
    "normal, not_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[(\"classifier\", LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'svm' : {\n",
    "        'model' : SVC(kernel='linear'),\n",
    "        'params' : {\n",
    "            'C' : [1, 10, 20],\n",
    "            'gamma' : ['scale', 'auto'],\n",
    "            'class_weight' : ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'random_forest' : {\n",
    "        'model' : RandomForestClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators' : [10, 100, 1000],\n",
    "            'max_features' : ['auto', 'sqrt', 'log2', 'none'],\n",
    "            'class_weight' : ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'logistic_regression' : {\n",
    "        'model' : LogisticRegression(solver = 'liblinear', multi_class='auto'),\n",
    "        'params' : {\n",
    "            'C' : np.linspace(1, 5, 10),\n",
    "            'penalty' : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "            'class_weight' : ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'bernoulliNB' : {\n",
    "        'model' : BernoulliNB(),\n",
    "        'params' : {\n",
    "            'alpha' : np.linspace(1, 10, 100)\n",
    "        }\n",
    "    },\n",
    "    'KN_neighbors' : {\n",
    "        'model' : KNeighborsClassifier(),\n",
    "        'params' : {\n",
    "            'weights' : ['uniform', 'distance'],\n",
    "            'algorithm' : ['auto', 'kd-tree', 'brute'],\n",
    "            'n_neighbors' : [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "        }\n",
    "    },\n",
    "    'MLP' : {\n",
    "        'model' : MLPClassifier(),\n",
    "        'params' : {}\n",
    "    },\n",
    "    'Decision_tree' : {\n",
    "        'model' : DecisionTreeClassifier(),\n",
    "        'params' : {\n",
    "            'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "            'splitter' : ['best', 'random'],\n",
    "            'max_features' : ['auto', 'sqrt', 'log2', 'none'],\n",
    "            'class_weight' : ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'extra_tree' : {\n",
    "        'model' : ExtraTreesClassifier(),\n",
    "        'params' : {\n",
    "            'n_estimators' : [10, 100, 1000],\n",
    "            'criterion' : ['gini', 'entropy', 'log_loss'],\n",
    "            'max_features' : ['sqrt', 'log2', 'none'],\n",
    "            'class_weight' : ['balanced']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\numpy\\core\\function_base.py:277: RuntimeWarning: overflow encountered in power\n",
      "  return _nx.power(base, y)\n"
     ]
    }
   ],
   "source": [
    "search_space = [{\"classifier\" : [LogisticRegression()],\n",
    "                 \"classifier__solver\" : ['sag', 'saga', 'liblinear'],\n",
    "                \"classifier__penalty\" : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                \"classifier__C\" : [np.logspace(1, 4, 10)],\n",
    "                \"classifier__class_weight\" : ['balanced', {\"0\" : 0.1, \"1\" : 0.9}]},\n",
    "                {\"classifier\" : [BernoulliNB()],\n",
    "                \"classifier__alpha\" : [np.linspace(1, 10, 100)]},\n",
    "                {\"classifier\" : [RandomForestClassifier()],\n",
    "                \"classifier__n_estimators\" : [10, 100, 1000],\n",
    "                \"classifier__criterion\" : ['gini', 'entropy', 'log_loss'],\n",
    "                \"classifier__max_features\" : ['auto', 'sqrt', 'log2', 'none'],\n",
    "                \"classifier__class_weight\" : ['balanced', [{0 : 1}, {1 : 9}]]},\n",
    "                {\"classifier\" : [KNeighborsClassifier()],\n",
    "                \"classifier__weights\" : ['uniform', 'distance'],\n",
    "                \"classifier__algorithm\" : ['auto', 'ball-tree', 'kd-tree', 'brute'],\n",
    "                \"classifier__n_neighbors\" : [np.linspace(1, 11, 6)]},\n",
    "                {\"classifier\" : [SVC()],\n",
    "                \"classifier__C\" : [np.logspace(1, 10000, 1000)],\n",
    "                \"classifier__class_weight\" : ['balanced', [{0 : 1}, {1 : 9}]]},\n",
    "                {\"classifier\" : [MLPClassifier()]},\n",
    "                {\"classifier\" : [DecisionTreeClassifier()],\n",
    "                \"classifier__criterion\" : ['gini', 'entropy', 'log_loss'],\n",
    "                \"classifier__splitter\" : ['best', 'random'],\n",
    "                \"classifier__max_features\" : ['auto', 'sqrt', 'log2', 'none'],\n",
    "                \"classifier__class_weight\" : ['balanced', [{0 : 1}, {1 : 9}]]},\n",
    "                {\"classifier\" : [ExtraTreesClassifier()],\n",
    "                \"classifier__n_estimators\" : [10, 100, 1000],\n",
    "                \"classifier__criterion\" : ['gini', 'entropy', 'log_loss'],\n",
    "                \"classifier__max_features\" : ['sqrt', 'log2', 'none'],\n",
    "                \"classifier__class_weight\" : ['balanced', [{0 : 1}, {1 : 9}]]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "splits = 3\n",
    "n_repeats = 10\n",
    "rskf = RepeatedStratifiedKFold(n_splits=splits, n_repeats=n_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "600 fits failed out of a total of 1200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 437, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 352, in _check_algorithm_metric\n",
      "    raise ValueError(\"unrecognized algorithm: '%s'\" % self.algorithm)\n",
      "ValueError: unrecognized algorithm: 'ball-tree'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 198, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 437, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 352, in _check_algorithm_metric\n",
      "    raise ValueError(\"unrecognized algorithm: '%s'\" % self.algorithm)\n",
      "ValueError: unrecognized algorithm: 'kd-tree'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.50435219 0.50435219 0.49995694 0.49995694 0.4994077  0.4994077\n",
      " 0.49975301 0.49975301 0.49995069 0.49995069        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.50435219 0.50435219 0.49995694 0.49995694 0.4994077  0.4994077\n",
      " 0.49975301 0.49975301 0.49995069 0.49995069]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "720 fits failed out of a total of 1440.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 281, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid value for max_features. Allowed string values are 'auto', 'sqrt' or 'log2'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.49951622 0.50293389 0.50369832 0.50918239 0.5074743  0.4981546\n",
      "        nan        nan 0.49190215 0.49664658 0.50321023 0.49516672\n",
      " 0.49637821 0.49913836        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.50295832 0.51025918 0.50471798 0.49505805 0.49612588 0.50163801\n",
      "        nan        nan 0.49386799 0.5087591  0.50679682 0.5074152\n",
      " 0.50413057 0.49527114        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "900 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 281, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid value for max_features. Allowed string values are 'auto', 'sqrt' or 'log2'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "360 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.49945701 0.5        0.5        0.49916027 0.5        0.5\n",
      "        nan        nan        nan 0.49951196 0.5        0.5\n",
      " 0.50035126 0.5        0.5               nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.50196496 0.5        0.5\n",
      " 0.50168571 0.5        0.5               nan        nan        nan\n",
      " 0.50010456 0.5        0.5        0.5003896  0.5        0.5\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "  clf = GridSearchCV(mp['model'], mp['params'], scoring = 'balanced_accuracy', cv = cv)\n",
    "  clf.fit(X_pca, y.values.flatten())\n",
    "  scores.append({\n",
    "      'model': model_name,\n",
    "      'best_score': clf.best_score_,\n",
    "      'best_params': clf.best_params_\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'KN_neighbors',\n",
       "  'best_score': 0.504352186550435,\n",
       "  'best_params': {'algorithm': 'auto',\n",
       "   'n_neighbors': 3,\n",
       "   'weights': 'uniform'}},\n",
       " {'model': 'MLP', 'best_score': 0.4988623231685379, 'best_params': {}},\n",
       " {'model': 'Decision_tree',\n",
       "  'best_score': 0.5102591770058341,\n",
       "  'best_params': {'class_weight': {0: 1, 1: 9},\n",
       "   'criterion': 'gini',\n",
       "   'max_features': 'auto',\n",
       "   'splitter': 'random'}},\n",
       " {'model': 'extra_tree',\n",
       "  'best_score': 0.5019649613424767,\n",
       "  'best_params': {'class_weight': {0: 1, 1: 9},\n",
       "   'criterion': 'gini',\n",
       "   'max_features': 'sqrt',\n",
       "   'n_estimators': 10}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scoring = [\"balanced_accuracy\", \"f1_weighted\"]\n",
    "\n",
    "search = GridSearchCV(pipe, search_space, cv=rskf, scoring=\"balanced_accuracy\", refit=True, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 210 candidates, totalling 6300 fits\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight=balanced, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l1, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=l2, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=elasticnet, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=sag;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=saga;, score=nan total time=   0.0s\n",
      "[CV 1/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 2/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 3/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 4/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 5/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 6/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 7/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 8/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 9/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 10/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 11/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 12/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 13/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 14/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 15/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 16/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 17/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 18/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 19/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 20/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 21/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 22/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 23/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 24/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 25/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 26/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 27/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 28/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 29/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n",
      "[CV 30/30] END classifier=LogisticRegression(), classifier__C=[   10.            21.5443469     46.41588834   100.\n",
      "   215.443469     464.15888336  1000.          2154.43469003\n",
      "  4641.58883361 10000.        ], classifier__class_weight={'0': 0.1, '1': 0.9}, classifier__penalty=none, classifier__solver=liblinear;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 11/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 12/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 13/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 14/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 15/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 16/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 17/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 18/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 19/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 20/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 21/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 22/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 23/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 24/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 25/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 26/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n",
      "[CV 27/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 28/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n",
      "[CV 29/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n",
      "[CV 30/30] END classifier=BernoulliNB(), classifier__alpha=[ 1.          1.09090909  1.18181818  1.27272727  1.36363636  1.45454545\n",
      "  1.54545455  1.63636364  1.72727273  1.81818182  1.90909091  2.\n",
      "  2.09090909  2.18181818  2.27272727  2.36363636  2.45454545  2.54545455\n",
      "  2.63636364  2.72727273  2.81818182  2.90909091  3.          3.09090909\n",
      "  3.18181818  3.27272727  3.36363636  3.45454545  3.54545455  3.63636364\n",
      "  3.72727273  3.81818182  3.90909091  4.          4.09090909  4.18181818\n",
      "  4.27272727  4.36363636  4.45454545  4.54545455  4.63636364  4.72727273\n",
      "  4.81818182  4.90909091  5.          5.09090909  5.18181818  5.27272727\n",
      "  5.36363636  5.45454545  5.54545455  5.63636364  5.72727273  5.81818182\n",
      "  5.90909091  6.          6.09090909  6.18181818  6.27272727  6.36363636\n",
      "  6.45454545  6.54545455  6.63636364  6.72727273  6.81818182  6.90909091\n",
      "  7.          7.09090909  7.18181818  7.27272727  7.36363636  7.45454545\n",
      "  7.54545455  7.63636364  7.72727273  7.81818182  7.90909091  8.\n",
      "  8.09090909  8.18181818  8.27272727  8.36363636  8.45454545  8.54545455\n",
      "  8.63636364  8.72727273  8.81818182  8.90909091  9.          9.09090909\n",
      "  9.18181818  9.27272727  9.36363636  9.45454545  9.54545455  9.63636364\n",
      "  9.72727273  9.81818182  9.90909091 10.        ];, score=nan total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 11/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 12/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 13/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 14/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 15/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 16/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.498 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 17/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 18/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 19/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 20/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 21/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 22/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 23/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.503 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 24/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 25/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 26/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.499 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 27/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.504 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 28/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 29/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.500 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 30/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=10;, score=0.503 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 11/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 12/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 13/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 14/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 15/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 16/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 17/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 18/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 19/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 20/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 21/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 22/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 23/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 24/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 25/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 26/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 27/30] END classifier=RandomForestClassifier(), classifier__class_weight=balanced, classifier__criterion=gini, classifier__max_features=auto, classifier__n_estimators=100;, score=0.500 total time=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Marta\\Desktop\\Studia\\CDV\\IV semestr 2022L\\Wykorzystanie Pythona w uczeniu maszynowym\\ml_project\\project\\ML_PROJECT_2022\\notebooks\\grid-search.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Marta/Desktop/Studia/CDV/IV%20semestr%202022L/Wykorzystanie%20Pythona%20w%20uczeniu%20maszynowym/ml_project/project/ML_PROJECT_2022/notebooks/grid-search.ipynb#ch0000010?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m search\u001b[39m.\u001b[39;49mfit(X_pca, y)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    893\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    839\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m         clone(base_estimator),\n\u001b[0;32m    841\u001b[0m         X,\n\u001b[0;32m    842\u001b[0m         y,\n\u001b[0;32m    843\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    844\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    845\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    846\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    847\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    848\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    849\u001b[0m     )\n\u001b[0;32m    850\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    851\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    852\u001b[0m     )\n\u001b[0;32m    853\u001b[0m )\n\u001b[0;32m    855\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\pipeline.py:394\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    393\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 394\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    451\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    452\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    453\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    454\u001b[0m )(\n\u001b[0;32m    455\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    456\u001b[0m         t,\n\u001b[0;32m    457\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    458\u001b[0m         X,\n\u001b[0;32m    459\u001b[0m         y,\n\u001b[0;32m    460\u001b[0m         sample_weight,\n\u001b[0;32m    461\u001b[0m         i,\n\u001b[0;32m    462\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    463\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    464\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    465\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    468\u001b[0m )\n\u001b[0;32m    470\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    938\u001b[0m         X,\n\u001b[0;32m    939\u001b[0m         y,\n\u001b[0;32m    940\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    941\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    942\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marta\\anaconda3\\envs\\python_in_ml\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = search.fit(X_pca, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python_in_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e3cc47668b018802626e3a416c19566715ed21c2b0a730c77eabf314b853671"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
